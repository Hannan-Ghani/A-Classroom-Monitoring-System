# -*- coding: utf-8 -*-
"""FINALYEARPROJECT.PYFILE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I0M98gNA5W7oqHkbTaDtq-xvtTStDN4B
"""

import numpy as np
import pickle
import cv2
import os
import model as embedding
from imutils import paths
import imutils

import argparse
import torch
import numpy as np
import torch.backends.cudnn as cudnn

from models.retinaface.config import cfg
from models.retinaface.prior_box import PriorBox
from models.retinaface.py_cpu_nms import py_cpu_nms

from models.retinaface.retinaface import RetinaFace
from models.retinaface.box_utils import decode , decode_landm
import time
                    
import torchvision.transforms.functional as F
from PIL import Image

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# paths to embedding pickle file
embeddingPickle = "./output/FinalEmbeddings.pickle"

# path to recognizer pickle file
recognizerPickle = "./output/FinalRecognizer.pickle"

# path to labels pickle file
labelPickle = "./output/FinalLabel.pickle"

predictedImg = "./predictedImg"

trained_model_path = "models/retinaface/weights/Final_Retinaface.pth"
cpu = False
confidence_threshold = 0.01
top_k = 5000
nms_threshold = 0.3
keep_top_k = 750
save_image_path = "predictedImg"
vis_threshold = 0.5

### check_keys

def check_keys(model, pretrained_state_dict):
    ckpt_keys = set(pretrained_state_dict.keys())
    model_keys = set(model.state_dict().keys())
    used_pretrained_keys = model_keys & ckpt_keys
    unused_pretrained_keys = ckpt_keys - model_keys
    missing_keys = model_keys - ckpt_keys
    print('Missing keys:{}'.format(len(missing_keys)))
    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))
    print('Used keys:{}'.format(len(used_pretrained_keys)))
    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'
    return True

### remove_prefix
def remove_prefix(state_dict, prefix):
    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''
    print('remove prefix \'{}\''.format(prefix))
    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x
    return {f(key): value for key, value in state_dict.items()}


### load_model
def load_model(model, pretrained_path, load_to_cpu):
    print('Loading pretrained model from {}'.format(pretrained_path))
    if load_to_cpu:
        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)
    else:
        device = torch.cuda.current_device()
        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))
    if "state_dict" in pretrained_dict.keys():
        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')
    else:
        pretrained_dict = remove_prefix(pretrained_dict, 'module.')
    check_keys(model, pretrained_dict)
    model.load_state_dict(pretrained_dict, strict=False)
    return model

torch.set_grad_enabled(False)

#net and model
net = RetinaFace(phase="test")
net = load_model(net , trained_model_path, cpu)
net.eval()
print("Finished loading model!")
cudnn.benchmark = True
device = torch.device("cpu" if cpu else "cuda")
net = net.to(device)
resize = 1



# load embedding model
embedder = embedding.InceptionResnetV1(pretrained="vggface2").eval()
#embedder = embedding.InceptionResnetV1(pretrained="casia-webface").eval()
# load the actual face recognition model along with the label encoder
recognizer = pickle.loads(open(recognizerPickle, "rb").read())
label = pickle.loads(open(labelPickle, "rb").read())
# loading embeddings pickle
data = pickle.loads(open(embeddingPickle, "rb").read())

COLORS = np.random.randint(0, 255, size=(len(label.classes_), 3), dtype="uint8")

labels = label.fit_transform(data["names"])

Embeddings = np.array(data["embeddings"])
names = np.array(data["names"])
print("Embeddings ", Embeddings.shape)
print("Names ", names.shape)
print("Labels ", labels.shape)


def distance(emb1, emb2):
    return np.sum(np.square(emb1 - emb2))

image_path = "1.jpg"
save_image = True

emotion_dict= {'Attentive': 0, 'Inattentive': 1}

from keras.models import load_model

model = load_model("/content/drive/My Drive/FYP_BRO/AttentiveInattentiveModel/my_model.hdf5")

path='/content/drive/My Drive/FYP_BRO/Face_Recognition/sample-videos/20200219_143025.mp4'

import pandas as pd

a=len(label.classes_)
arr = np.empty((10000, a), dtype=object)

j=0

while(a!=j):
  arr[0,j]= label.classes_[j]
  print(label.classes_[j])
  j+=1


path='/content/drive/My Drive/FYP_BRO/Face_Recognition/sample-videos/v9.wmv'

  import numpy as np
  import cv2
  from google.colab.patches import cv2_imshow
  cap = cv2.VideoCapture(path) #video_name is the video being called
  frame_seconds=0
  frame_no=0 
  attentivecount=0
  inattentivecount=0
  frame_width = int(cap.get(3))
  frame_height = int(cap.get(4))
  out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))

  while (cap.isOpened()):
      cap.set(1,frame_seconds); # Where frame_no is the frame you want
      frame_seconds+=60
      frame_no+=1

      ret, frame = cap.read() # Read the frame
      if not ret:
            print("Can't receive frame (stream end?). Exiting ...")
            break
      #cv2_imshow(frame) # show frame on window
      image_path = frame
      #img_raw = cv2.imread(image_path, cv2.IMREAD_COLOR)
      img_raw = frame
      img_raw_rgb = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)

      imageName = path.split('/')[-1].split('.')[-2]
      frmn=str(frame_no)
      frms=str(frame_seconds)
      imageName = imageName+' : '+' '+frmn+' : '+frms
      img = np.float32(img_raw)

      im_height, im_width, _ = img.shape
      scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])
      img -= (104, 117, 123)
      img = img.transpose(2, 0, 1)
      img = torch.from_numpy(img).unsqueeze(0)
      img = img.to(device)
      scale = scale.to(device)

      tic = time.time()
      loc, conf, landms = net(img)  # forward pass
      print('net forward time: {:.4f}'.format(time.time() - tic))

      priorbox = PriorBox(cfg, image_size=(im_height, im_width))
      priors = priorbox.forward()
      priors = priors.to(device)
      prior_data = priors.data
      boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])
      boxes = boxes * scale / resize
      boxes = boxes.cpu().numpy()
      scores = conf.squeeze(0).data.cpu().numpy()[:, 1]
      landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])
      scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],
                                    img.shape[3], img.shape[2], img.shape[3], img.shape[2],
                                    img.shape[3], img.shape[2]])
      scale1 = scale1.to(device)
      landms = landms * scale1 / resize
      landms = landms.cpu().numpy()

      # ignore low scores
      inds = np.where(scores > confidence_threshold)[0]
      boxes = boxes[inds]
      landms = landms[inds]
      scores = scores[inds]

      # keep top-K before NMS
      order = scores.argsort()[::-1][:top_k]
      boxes = boxes[order]
      landms = landms[order]
      scores = scores[order]

      # do NMS
      dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
      keep = py_cpu_nms(dets, nms_threshold)
      # keep = nms(dets, args.nms_threshold,force_cpu=args.cpu)
      dets = dets[keep, :]
      landms = landms[keep]

      # keep top-K faster NMS
      dets = dets[:keep_top_k, :]
      landms = landms[:keep_top_k, :]

      dets = np.concatenate((dets, landms), axis=1)
      #############################
      faces_counter=0
      frame_number=0
      result = {}
      faces_info = {}
      faces_name = {}
      face_state = {}
      no_of_faces_recognized=0
      Attentive=0
      Inattentive=0
      textx=im_width/2
      texty=im_height/2
      #######################

      # show image
      if save_image:
                  for b in dets:
                      faces_counter= faces_counter+1
                      if b[4] < vis_threshold:
                          continue
                      boxes = np.array(b[0:4])
                      boxes = boxes.astype('int')
                      #attempt to save detected faces to a csv file
                      frame_number += 1
                
                      ###############################
                      (startX , startY, endX, endY) = boxes

                      face = img_raw_rgb[startY:endY , startX:endX]

                      try:
                          #print("yes-1")
                          faceRead = Image.fromarray(face)
                          faceRead = faceRead.resize((160, 160), Image.ANTIALIAS)
                          faceRead = F.to_tensor(faceRead)
                          #print("yes-2")
                      except:
                          print("[Error] - resizing face ")
                          continue
                      #print(faceRead.shape)

                      # getting embeddings for croped faces
                      faceEmbed = embedder(faceRead.unsqueeze(0))
                      flattenEmbed = faceEmbed.squeeze(0).detach().numpy()
                      #print(flattenEmbed.shape)

                      # predicting class
                      array = np.array(flattenEmbed).reshape(1,-1)
                      # perform classification to recognize the face
                      preds = recognizer.predict_proba(array)[0]
                      face_image = cv2.resize(face, (64,64))
                      face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
                      face_image = np.reshape(face_image, [1, face_image.shape[0], face_image.shape[1], 1])
                      j = np.argmax(preds)
                      a= np.argmax(model.predict(face_image))
                      label_map = dict((v,k) for k,v in emotion_dict.items()) 
                      proba = preds[j]
                      name = label.classes_[j]
                      emotion = label_map[a]
                      att='Attentive'
                      inatt='Inattentive'
                      if (a=='Attentive'):
                        Attentive+=1
                      elif (a=='Inattentive'):
                        Inattentive+=1


                      #print(name)

                      result = np.where(names == name)
                      resultEmbeddings = Embeddings[result]

                      dists = []
                      for emb in resultEmbeddings:
                          d = distance(emb, flattenEmbed)
                          dists.append(d)
                      #print(dists)
                      distarray = np.array(dists)
                      #print(distarray)
                      min_dist = np.min(distarray)
                      max_dist = np.max(distarray)
                      #print("Name : ",name);lopo;
                      #print("min dist : ",min_dist)
                      #print("max dist : ", max_dist)
                      
                      if proba >= 0.15:

                          if (min_dist < 0.8 and max_dist <= 1.3) or (proba >= 0.40):

                              #print("dist name ", name)
                              #print("min dist : ",min_dist)
                              #print("max dist : ", max_dist)

                              i=1
                              j=0
                              while(j<20):
                                  
                                  if (arr[0,j]==name and i<100):
                                    while arr[i,j]!=None:
                                      i+=1
                                    if arr[i,j]==None:
                                      arr[i,j]=emotion
                                      print('Detected: wewewewe', name, emotion)
                                      #i+=1
                                    #elif arr[i,j]!=None:
                                    #  i+=1
                                  print('Detected: ', name, emotion,i,j)
                                  j+=1

                              #color = [int(c) for c in COLORS[j]]
                              color = (0, 255, 0)
                              cv2.rectangle(img_raw, (startX, startY), (endX, endY), color, 2)

                              #text = "{}: {:.2f}".format(name, proba)
                              text = "{}/{}/{:.2f}".format(name, emotion,proba)
                              #text = "{}/{}".format(name, emotion)

                              cv2.putText(img_raw,text, (startX, startY - 5),  cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                              ###########################
                              if (emotion=='Attentive'):
                                Attentive+=1
                              elif (emotion=='Inattentive'):
                                Inattentive+=1
                                                  
                              faces_name[faces_counter] = name + ' : '+ emotion
                              face_state[faces_counter] = emotion
                              #########################

                          else:

                              name = "NONE"

                              color = (255, 255, 255)

                              cv2.rectangle(img_raw, (startX, startY), (endX, endY), color, 2)

                              text = "{}".format(emotion)
                              #text = "{}/{}/{:.2f}".format(name, emotion,proba)
                              #text = "{}/{}".format(name, emotion)
                              cv2.putText(img_raw,text, (startX, startY - 5),  cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                              #####################
                              if (emotion=='Attentive'):
                                Attentive+=1
                              elif (emotion=='Inattentive'):
                                Inattentive+=1                    

                              faces_name[faces_counter] = name + ' : '+ emotion
                              face_state[faces_counter] = emotion
                              ####################

                      else:

                          name = "NONE"

                          color = (255, 255, 255)

                          cv2.rectangle(img_raw, (startX, startY), (endX, endY), color, 2)

                          text = "{}".format(emotion)
                          #text = "{}/{}/{:.2f}".format(name, emotion,proba)

                          cv2.putText(img_raw,text, (startX, startY - 5),  cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                          #####################
                          if (emotion=='Attentive'):
                            Attentive+=1
                          elif (emotion=='Inattentive'):
                            Inattentive+=1                   

                          faces_name[faces_counter] = name + ' : '+ emotion
                          face_state[faces_counter] = emotion
                          #tems[face_counter]= name + emotion
                          ####################                        
      Total_faces=Attentive+Inattentive
      attentivecount+=Attentive
      inattentivecount+=Inattentive
      
      T_f=str(Total_faces)
      Att=str(Attentive)
      Inatt=str(Inattentive)

      strr='Total Faces Detected: '+T_f+' Attentive :'+Att+' Inattentive :'+Inatt

      # save image predict folder
      font = cv2.FONT_HERSHEY_COMPLEX
      cv2.putText(img_raw,strr, (5, 15),  font, 0.5, (255,255,255), 1)
      #cv2.putText(img_raw,Total_faces, (0,0), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)
      df = pd.DataFrame(faces_name.items(),columns = ['Number','Detected_Students'])
      csvfolder='CSV/'
      filename=imageName+'.csv'
      df.to_csv(csvfolder+filename,index = False)
      ######################
      #cv2.putText(predictedImg,, (textx, texty - 5),  cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
      cv2.imwrite("{}/{}.png".format(predictedImg, imageName), img_raw)
      im = Image.open("{}/{}.png".format(predictedImg,imageName))
      out.write(img_raw)
      if cv2.waitKey(1) & 0xFF == ord('q'):
        break
      cv2_imshow(img_raw)
      

      
      #draw = ImageDraw.Draw(im)
      #font = ImageFont.truetype(<font-file>, <font-size>)
      #font = ImageFont.truetype("sans-serif.ttf", 16)
      #draw.text((x, y),"Sample Text",(r,g,b))
      #draw.text((5, 15),strr,(255,255,255),font=font)
      #im.save('sample-out.jpg')
      
      #plt.imshow(img_raw)
      #plt.show()
    #TotalClassAttentivePercentage=0
    #TotalClassInattentivePercentage=0
    #a=int(TotalClassAttentivePercentage)
    #b=int(TotalClassInattentivePercentage)
  cap.release()
  out.release()
  cv2.destroyAllWindows() 
  
  facescount=attentivecount+inattentivecount
  atperc=(attentivecount/(attentivecount+inattentivecount))*100
  iatperc=(inattentivecount/(attentivecount+inattentivecount))*100
  print('Total_Frames : ',frame_no,'Total Faces :',facescount,'Attentive',attentivecount,'Inattentive',inattentivecount)
    
  print('Attentive % : ',atperc,'Inattentive % : ',iatperc)


counter=0
s=0
while(s<len(label.classes_)):
  if (arr[1,s]!=None): 
     counter+=1
  s+=1


arrb = np.zeros((3, counter), dtype=object)


counter=0
s=0
while(s<len(label.classes_)):
  if (arr[1,s]!=None): 
     arrb[0,counter]=arr[0,s] 
     counter+=1
  s+=1



i=0
s=0
act=0
ict=0
while i<counter :
  act=0
  ict=0
  while s<len(label.classes_):
    if i<counter:
     if arrb[0,i]==arr[0,s] :
      print(arr[0,s])
      j=1
      while arr[j,s]!=None:
        if arr[j,s]=='Attentive' and arrb[0,i]==arr[0,s]:
          act+=1
          arrb[1,i]=act
          print(arr[0,s],arrb[0,i],'att',act)
        elif arr[j,s]=='Inattentive' and arrb[0,i]==arr[0,s]:
          ict+=1
          arrb[2,i]=ict
          print(arr[0,s],arrb[0,i],'iatt',ict)
        j+=1
      #arrb[1,i]=act
      #arrb[2,i]=ict
      i+=1
      act=0
      ict=0
    s+=1 
  #i+=1 


print('Total_Frames : ',frame_no,'Total Faces :',facescount,'Attentive',attentivecount,'Inattentive',inattentivecount)   
print('Attentive % : ',atperc,'Inattentive % : ',iatperc)

arrc = np.zeros((1, 6), dtype=object)

arrc[0,0]=frame_no
arrc[0,1]=facescount
arrc[0,2]=attentivecount
arrc[0,3]=inattentivecount
arrc[0,4]=atperc
arrc[0,5]=iatperc


df = pd.DataFrame(arrc, columns=['Total_Frames','Total Faces','Attentive','Inattentive','Attentive %','Inattentive %'])
df.to_csv('/content/collective_data.csv',index = False)

arrb= arrb.T

import numpy

import pandas as pd 
pd.DataFrame(arrb).to_csv("/content/sample_data.csv")

df = pd.DataFrame(arrb, columns=['Students','Attentive','Inattentive'])
df.to_csv('/content/sample_data.csv',index = False)

pd.DataFrame.columns