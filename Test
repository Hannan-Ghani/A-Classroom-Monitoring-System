import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SparkSession, Row}
import org.apache.spark.sql.functions._
import org.apache.log4j.Logger
import com.amazonaws.services.glue.util.GlueArgParser
import java.io.{File, PrintWriter}
import scala.collection.mutable.ListBuffer

object DynamicCaseClass {

  // Initialize logger globally
  val logger: Logger = Logger.getLogger(this.getClass.getName)

  def main(sysArgs: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder().getOrCreate()
    val glueContext: GlueContext = new GlueContext(spark.sparkContext)
    import spark.implicits._

    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3PathConfig", "s3ValidationConfig").toArray)
    val s3PathConfig = args("s3PathConfig")
    val s3ValidationConfig = args("s3ValidationConfig")

    val pathConfig = readPathConfig(spark.read.textFile(s3PathConfig).collect())
    val validationConfig = parseValidationConfig(spark.read.textFile(s3ValidationConfig).collect())
    val inputSourcePath = pathConfig.getOrElse("inputSourcePath", "")
    val inputTargetPath = pathConfig.getOrElse("inputTargetPath", "")
    val outputBasePath = pathConfig.getOrElse("outputBasePath", "")

    val transactionSource: DataFrame = spark.read.parquet(inputSourcePath)
    val caseClass: DataFrame = spark.read.parquet(inputTargetPath)

    var combinedResults: DataFrame = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], caseClass.schema)
    val summaryLog = new ListBuffer[String]

    // Null Validation
    if (validationConfig.contains("null_validation")) {
      val nullColumns = validationConfig("null_validation")("columns")
      val nullValidationResult = applyNullValidation(nullColumns, caseClass)(spark)
      combinedResults = mergeWithCombinedResults(combinedResults, nullValidationResult)
      summaryLog += s"Null Validation: ${nullValidationResult.count()} unmatched rows"
    }

    // Direct Column Validation
    if (validationConfig.contains("direct_column_validation")) {
      val sourceCols = validationConfig("direct_column_validation")("columns_source")
      val targetCols = validationConfig("direct_column_validation")("columns_target")
      val (directValidationResult, unmatchedIds) = applyDirectColumnValidation(sourceCols, targetCols, transactionSource, caseClass)(spark)
      combinedResults = mergeWithCombinedResults(combinedResults, directValidationResult)
      summaryLog += s"Direct Column Validation: ${directValidationResult.count()} unmatched rows"
      logTransactionIds(unmatchedIds, "Direct Column Validation", outputBasePath)
    }

    // Narrative Validation
    if (validationConfig.contains("narrative_validation")) {
      val sourceNarrativeCol = validationConfig("narrative_validation")("source_column")
      val targetNarrativeCol = validationConfig("narrative_validation")("target_column")
      val (narrativeValidationResult, unmatchedIds) = applyNarrativeValidation(sourceNarrativeCol, targetNarrativeCol, transactionSource, caseClass)(spark)
      combinedResults = mergeWithCombinedResults(combinedResults, narrativeValidationResult)
      summaryLog += s"Narrative Validation: ${narrativeValidationResult.count()} unmatched rows"
      logTransactionIds(unmatchedIds, "Narrative Validation", outputBasePath)
    }

    // Amount Local Validation
    if (validationConfig.contains("amount_local_validation")) {
      val sourceAmountCol = validationConfig("amount_local_validation")("source_column")
      val targetAmountCol = validationConfig("amount_local_validation")("target_column")
      val (amountLocalValidationResult, unmatchedIds) = applyAmountLocalValidation(sourceAmountCol, targetAmountCol, transactionSource, caseClass)(spark)
      combinedResults = mergeWithCombinedResults(combinedResults, amountLocalValidationResult)
      summaryLog += s"Amount Local Validation: ${amountLocalValidationResult.count()} unmatched rows"
      logTransactionIds(unmatchedIds, "Amount Local Validation", outputBasePath)
    }

    // Transaction Country ISO3 Validation
    if (validationConfig.contains("transaction_country_iso3_validation")) {
      val sourceCountryCol = validationConfig("transaction_country_iso3_validation")("source_column")
      val targetCountryCol = validationConfig("transaction_country_iso3_validation")("target_column")
      val (countryIsoValidationResult, unmatchedIds) = applyTransactionCountryISO3Validation(sourceCountryCol, targetCountryCol, transactionSource, caseClass)(spark)
      combinedResults = mergeWithCombinedResults(combinedResults, countryIsoValidationResult)
      summaryLog += s"Transaction Country ISO3 Validation: ${countryIsoValidationResult.count()} unmatched rows"
      logTransactionIds(unmatchedIds, "Transaction Country ISO3 Validation", outputBasePath)
    }

    val combinedOutputPath = s"${outputBasePath}/combined_validation_output/"
    combinedResults.write.mode("overwrite").parquet(combinedOutputPath)
    writeSummaryLog(summaryLog.toList, s"${outputBasePath}/summary_log.txt")

    Job.commit()
  }

  def logTransactionIds(transactionIds: List[String], validationType: String, outputBasePath: String): Unit = {
    val outputFile = new File(s"${outputBasePath}/${validationType}_unmatched_ids.txt")
    val writer = new PrintWriter(outputFile)
    writer.write(s"$validationType Unmatched Transaction IDs:\n")
    transactionIds.foreach(id => writer.write(s"$id\n"))
    writer.close()
  }

  def writeSummaryLog(log: List[String], outputPath: String): Unit = {
    val outputFile = new File(outputPath)
    val writer = new PrintWriter(outputFile)
    log.foreach(line => writer.write(s"$line\n"))
    writer.close()
  }

  def readPathConfig(config: Array[String]): Map[String, String] = {
    config.filterNot(line => line.trim.isEmpty || line.trim.startsWith("#")).flatMap { line =>
      line.split("=").map(_.trim) match {
        case Array(key, value) if key.nonEmpty && value.nonEmpty =>
          Some(key -> value)
        case _ =>
          logger.warn(s"Invalid line in path config: $line")
          None
      }
    }.toMap
  }

  def parseValidationConfig(config: Array[String]): Map[String, Map[String, String]] = {
    var currentSection: String = ""
    var validationConfig: Map[String, Map[String, String]] = Map()
    config.filterNot(line => line.trim.isEmpty || line.trim.startsWith("#")).foreach { line =>
      if (line.startsWith("[") && line.endsWith("]")) {
        currentSection = line.substring(1, line.length - 1).trim
        validationConfig += (currentSection -> Map())
      } else {
        val keyValue = line.split("=").map(_.trim)
        if (keyValue.length == 2 && currentSection.nonEmpty) {
          val currentValues = validationConfig(currentSection)
          validationConfig += (currentSection -> (currentValues + (keyValue(0) -> keyValue(1))))
        } else {
          logger.warn(s"Invalid line in validation config: $line")
        }
      }
    }
    validationConfig
  }

  def applyNullValidation(columns: String, caseClass: DataFrame)(implicit spark: SparkSession): DataFrame = {
    import spark.implicits._
    if (!caseClass.columns.contains("TRANSACTION_ID")) {
      logger.warn("TRANSACTION_ID is missing. Skipping null validation.")
      return spark.createDataFrame(spark.sparkContext.emptyRDD[Row], caseClass.schema)
    }
    val nullColumns = columns.split(",").map(_.trim).toSeq
    caseClass.select(nullColumns.map(col): _*).distinct().withColumn("ValidationType", lit("Null Validation"))
  }

  def applyDirectColumnValidation(sourceCols: String, targetCols: String, transactionSource: DataFrame, caseClass: DataFrame)(implicit spark: SparkSession): (DataFrame, List[String]) = {
    import spark.implicits._
    if (!transactionSource.columns.contains("TRANSACTION_ID") || !caseClass.columns.contains("TRANSACTION_ID")) {
      logger.warn("TRANSACTION_ID is missing in either source or target. Skipping direct column validation.")
      return (spark.createDataFrame(spark.sparkContext.emptyRDD[Row], caseClass.schema), List.empty)
    }
    val sourceColumns = sourceCols.split(",").map(_.trim).toSeq
    val targetColumns = targetCols.split(",").map(_.trim).toSeq

    val (existingSourceColumns, existingTargetColumns) = filterExistingColumnPairs(sourceColumns, targetColumns, transactionSource, caseClass)
    if (existingSourceColumns.isEmpty || existingTargetColumns.isEmpty) {
      logger.warn(s"No valid columns left for comparison after filtering missing columns.")
      (spark.createDataFrame(spark.sparkContext.emptyRDD[Row], caseClass.schema), List.empty)
    } else {
      val sourceDirectData = transactionSource.select($"TRANSACTION_ID" +: existingSourceColumns.map(col): _*)
      val targetDirectData = caseClass.select($"transactionid" +: existingTargetColumns.map(col): _*)

      val directSourceToTargetDiff = sourceDirectData.exceptAll(targetDirectData)
      val directTargetToSourceDiff = targetDirectData.exceptAll(sourceDirectData)
      val unmatchedIds = directSourceToTargetDiff.select("TRANSACTION_ID").as[String].collect().toList

      val result = directSourceToTargetDiff.withColumn("ValidationType", lit("Direct Source-to-Target Validation"))
        .unionByName(directTargetToSourceDiff.withColumn("ValidationType", lit("Direct Target-to-Source Validation")), allowMissingColumns = true)

      (result, unmatchedIds)
    }
  }

  def applyNarrativeValidation(sourceNarrativeCol: String, targetNarrativeCol: String, transactionSource: DataFrame, caseClass: DataFrame)(implicit spark: SparkSession): (DataFrame, List[String]) = {
    import spark.implicits._
    if (!transactionSource.columns.contains("TRANSACTION_ID") || !caseClass.columns.contains("TRANSACTION_ID")) {
      logger.warn("TRANSACTION_ID is missing in either source or target. Skipping narrative validation.")
      return (spark.createDataFrame(spark.sparkContext.emptyRDD[Row], caseClass.schema), List.empty)
    }
    val sourceNarrativeData = transactionSource.select($"TRANSACTION_ID", col(sourceNarrativeCol))
    val targetNarrativeData = caseClass.select($"transactionid", col(targetNarrativeCol))

    val narrativeSourceToTargetDiff = sourceNarrativeData.exceptAll(targetNarrativeData)
    val narrativeTargetToSourceDiff = targetNarrativeData.exceptAll(sourceNarrativeData)
    val unmatchedIds = narrativeSourceToTargetDiff.select("TRANSACTION_ID").as[String].collect().toList

    val result = narrativeSourceToTargetDiff.withColumn("ValidationType", lit("Narrative Source-to-Target Validation"))
      .unionByName(narrativeTargetToSourceDiff.withColumn("ValidationType", lit("Narrative Target-to-Source Validation")), allowMissingColumns = true)

    (result, unmatchedIds)
  }

  def applyAmountLocalValidation(sourceAmountCol: String, targetAmountCol: String, transactionSource: DataFrame, caseClass: DataFrame)(implicit spark: SparkSession): (DataFrame, List[String]) = {
    import spark.implicits._
    if (!transactionSource.columns.contains("TRANSACTION_ID") || !caseClass.columns.contains("TRANSACTION_ID")) {
      logger.warn("TRANSACTION_ID is missing in either source or target. Skipping amount local validation.")
      return (spark.createDataFrame(spark.sparkContext.emptyRDD[Row], caseClass.schema), List.empty)
    }
    val sourceAmountData = transactionSource.select($"TRANSACTION_ID", col(sourceAmountCol))
    val targetAmountData = caseClass.select($"transactionid", col(targetAmountCol))

    val amountLocalSourceToTargetDiff = sourceAmountData.exceptAll(targetAmountData)
    val amountLocalTargetToSourceDiff = targetAmountData.exceptAll(sourceAmountData)
    val unmatchedIds = amountLocalSourceToTargetDiff.select("TRANSACTION_ID").as[String].collect().toList

    val result = amountLocalSourceToTargetDiff.withColumn("ValidationType", lit("Amount Local Source-to-Target Validation"))
      .unionByName(amountLocalTargetToSourceDiff.withColumn("ValidationType", lit("Amount Local Target-to-Source Validation")), allowMissingColumns = true)

    (result, unmatchedIds)
  }

  def applyTransactionCountryISO3Validation(sourceCountryCol: String, targetCountryCol: String, transactionSource: DataFrame, caseClass: DataFrame)(implicit spark: SparkSession): (DataFrame, List[String]) = {
    import spark.implicits._
    if (!transactionSource.columns.contains("TRANSACTION_ID") || !caseClass.columns.contains("TRANSACTION_ID")) {
      logger.warn("TRANSACTION_ID is missing in either source or target. Skipping transaction country ISO3 validation.")
      return (spark.createDataFrame(spark.sparkContext.emptyRDD[Row], caseClass.schema), List.empty)
    }
    val mappingUDF = udf((input: String) => Map("AD" -> "AND", "AE" -> "ARE", "AF" -> "AFG", "AG" -> "ATG", "AI" -> "AIA").getOrElse(input, input))
    val sourceTransactionCountry = transactionSource.withColumn("transactionCountryIso3Mapped", mappingUDF(col(sourceCountryCol)))
    val targetTransactionCountry = caseClass.select(col(targetCountryCol))

    val countryIsoSourceToTargetDiff = sourceTransactionCountry.select($"TRANSACTION_ID", col("transactionCountryIso3Mapped")).exceptAll(targetTransactionCountry)
    val countryIsoTargetToSourceDiff = targetTransactionCountry.exceptAll(sourceTransactionCountry.select($"TRANSACTION_ID", col("transactionCountryIso3Mapped")))
    val unmatchedIds = countryIsoSourceToTargetDiff.select("TRANSACTION_ID").as[String].collect().toList

    val result = countryIsoSourceToTargetDiff.withColumn("ValidationType", lit("Transaction Country ISO3 Source-to-Target Validation"))
      .unionByName(countryIsoTargetToSourceDiff.withColumn("ValidationType", lit("Transaction Country ISO3 Target-to-Source Validation")), allowMissingColumns = true)

    (result, unmatchedIds)
  }

  def filterExistingColumnPairs(sourceColumns: Seq[String], targetColumns: Seq[String], transactionSource: DataFrame, caseClass: DataFrame): (Seq[String], Seq[String]) = {
    val existingSourceColumns = filterExistingColumns(transactionSource, sourceColumns)
    val existingTargetColumns = filterExistingColumns(caseClass, targetColumns)
    sourceColumns.zip(targetColumns).foreach { case (sourceCol, targetCol) =>
      if (!existingSourceColumns.contains(sourceCol) || !existingTargetColumns.contains(targetCol)) {
        logger.warn(s"Skipping comparison for columns: $sourceCol and $targetCol because one or both are missing.")
      }
    }
    val validPairs = sourceColumns.zip(targetColumns).filter { case (sourceCol, targetCol) =>
      existingSourceColumns.contains(sourceCol) && existingTargetColumns.contains(targetCol)
    }
    (validPairs.map(_._1), validPairs.map(_._2))
  }

  def filterExistingColumns(df: DataFrame, columns: Seq[String]): Seq[String] = {
    val existingColumns = df.columns.toSet
    columns.filter(existingColumns.contains)
  }

  def mergeWithCombinedResults(existingResults: DataFrame, newResults: DataFrame): DataFrame = {
    if (existingResults.isEmpty) newResults else existingResults.unionByName(newResults, allowMissingColumns = true)
  }
}