import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum

# Initialize Glue Context
glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session

# Load transaction data
transaction_date_source = spark.read.parquet("s3://136731789529-fis-raw-data/transaction/*/raw/parquet/*")
transaction_count = transaction_date_source.count()  # Counting rows (stored but not displayed)

# Select specific columns
source = transaction_date_source.select(
    col("customer_key"),
    col("account_key"),
    col("execution_date"),
    col("transaction_date"),
    col("original_amount"),
    col("original_currency_code"),
    col("transaction_location"),
    col("transaction_country"),
    col("transaction_country_iso_code"),
    col("narrative"),
    col("shortnarrative"),
    col("transaction_type"),
    col("txn_direction"),
    col("txn_direction_desc"),
    col("transaction_type_desc"),
    col("amount"),
    col("transaction_id"),
    col("transaction_code"),
    col("transaction_code_desc"),
    col("sterling_equivalent"),
    col("int_dollar_equivalent"),
    col("transaction_routing_number"),
    col("external_account_number"),
    col("counterparty_customer_key"),
    col("counterparty_product_key"),
    col("counterparty_id"),
    col("source_transaction_id"),
    col("branch_key"),
    col("account_bucket"),
    col("jurisdiction"),
    col("screening_system"),
    col("source_system"),
    col("EFF_START_DATE")
)

# Reading another transaction source
transaction_source = spark.read.parquet("s3://136731789529-fis-raw-data/transaction/*/raw/parquet/*")
transaction_source_count = transaction_source.count()  # Counting rows (stored but not displayed)

# Reading CSV data with specific options
data = spark.read.option("header", "true").option("delimiter", "|").csv("s3://136731789529-fis-raw-data/transaction/*/raw/parquet/qtx_common_*")
data_master = data.agg(sum(col("Count").cast("long"))).first()[0]  # Aggregating count (stored but not displayed)

# Reading a specific parquet file and counting rows
parquet_20230914 = spark.read.parquet("s3://136731789529-fis-raw-data/transaction/*/raw/parquet/*")
parquet_20230914_count = parquet_20230914.count()  # Counting rows (stored but not displayed)

# Reading metadata file
metadata_file = spark.read.parquet("s3://136731789529-fis-interim-data/transaction/metadata.parquet/")

# Filtering audit summary data
audit_summary = spark.read.parquet("s3://136731789529-fis-interim-data/auditSummary/quantexaAuditSummary.parquet/*") \
    .filter(col("batchId").isin("2023-10-23_13-03-31-472")) \
    .filter(~col("processingStage").isin("InputRawToPartition", "S3RawLayer"))