To dynamically integrate the provided transformation into your application and call it from the main function, I’ll encapsulate the logic into a function, ensure proper validation for column existence, and dynamically compare the transformed resultDF with the target.

Transformation Function

Here’s how the transformation logic is encapsulated into a reusable function:

def validateCountryIsoMapping(
    transactionSource: DataFrame,
    caseClass: DataFrame
): Seq[String] = {
  import transactionSource.sparkSession.implicits._

  // Define the mapping UDF
  val mappingUDF = udf((input: String) =>
    Map(
      "AD" -> "AND",
      "AE" -> "ARE",
      "TK" -> "TKL",
      "YG" -> "SRB"
    ).getOrElse(input, input)
  )

  // Check if the required column exists in the transactionSource DataFrame
  if (!transactionSource.columns.contains("TRANSACTION_COUNTRY_ISO_CODE")) {
    return Seq("Validation Skipped: TRANSACTION_COUNTRY_ISO_CODE column not found in transactionSource.")
  }

  if (!caseClass.columns.contains("transactionCountryIso3")) {
    return Seq("Validation Skipped: transactionCountryIso3 column not found in caseClass.")
  }

  // Perform the transformation
  val resultDF = transactionSource
    .withColumn("newISO3", mappingUDF(col("TRANSACTION_COUNTRY_ISO_CODE")))
    .select($"TRANSACTION_ID", $"newISO3")

  val target = caseClass.select($"transactionId", $"transactionCountryIso3")

  // Compute differences
  val sourceToTargetDiff = resultDF.exceptAll(target)
  val targetToSourceDiff = target.exceptAll(resultDF)

  val sourceToTargetIds = sourceToTargetDiff.select($"TRANSACTION_ID").as[String].collect()
  val targetToSourceIds = targetToSourceDiff.select($"transactionId").as[String].collect()

  val sourceToTargetCount = sourceToTargetIds.length
  val targetToSourceCount = targetToSourceIds.length

  // Generate results
  Seq(
    "Country ISO Mapping Validation Results:",
    s"Source to Target Differences: $sourceToTargetCount rows found " + sourceToTargetIds.mkString(", "),
    s"Target to Source Differences: $targetToSourceCount rows found " + targetToSourceIds.mkString(", ")
  )
}

Explanation of the Code

	1.	Mapping UDF:
	•	A Spark UDF (mappingUDF) is defined to map ISO country codes (TRANSACTION_COUNTRY_ISO_CODE) to their corresponding values or return the input value if no mapping exists.
	2.	Column Existence Validation:
	•	Checks if TRANSACTION_COUNTRY_ISO_CODE exists in transactionSource and if transactionCountryIso3 exists in caseClass.
	•	If any column is missing, it returns a message and skips the validation.
	3.	Transformation:
	•	A new column newISO3 is created using the mapping UDF.
	•	Only the columns TRANSACTION_ID and newISO3 are selected for comparison.
	4.	Comparison:
	•	resultDF and target DataFrames are compared using exceptAll to find differences.
	•	Differences are collected and returned as a sequence of strings.

How to Call in main Function

You can call this function in the main method as follows:

validationResults = validationResults ++ validateCountryIsoMapping(transactionSource, caseClass)

Complete Flow in main Function

Here’s how the main function would look with this added validation:

def main(sysArgs: Array[String]): Unit = {
  val sparkSession: SparkSession = SparkSession.builder.getOrCreate()
  val glueContext: GlueContext = new GlueContext(sparkSession.sparkContext)
  import sparkSession.implicits._

  val logger: Logger = Logger.getLogger("CleanseCaseClassLogger")

  // Get Glue job parameters
  val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3PathConfig", "s3ValidationConfig").toArray)
  val s3PathConfig = args("s3PathConfig")
  val s3ValidationConfig = args("s3ValidationConfig")

  // Read configuration files
  val pathConfigMap = parseConfigFile(s3PathConfig, sparkSession)
  val validationConfigMap = parseValidationConfig(s3ValidationConfig, sparkSession)

  // Extract paths
  val inputSourcePath = pathConfigMap("inputSourcePath")
  val inputTargetPath = pathConfigMap("inputTargetPath")
  val outputBasePath = pathConfigMap("outputBasePath")

  // Load data
  val transactionSource: DataFrame = sparkSession.read.parquet(inputSourcePath)
  val caseClass: DataFrame = sparkSession.read.parquet(inputTargetPath)

  // Initialize validation results
  var validationResults = Seq[String]()

  // Null Validation
  validationResults = validationResults ++ validateNullColumns(caseClass, validationConfigMap.getOrElse("null_validation", Seq()))

  // Direct Column Validation
  validationResults = validationResults ++ validateDirectColumns(transactionSource, caseClass, validationConfigMap)

  // Narrative Validation
  validationResults = validationResults ++ validateNarrativeColumns(transactionSource, caseClass, validationConfigMap)

  // Country ISO Mapping Validation
  validationResults = validationResults ++ validateCountryIsoMapping(transactionSource, caseClass)

  // Convert results to DataFrame and write to S3
  val resultsDF = validationResults.toDF("validation_result")
  resultsDF.write.mode("overwrite").text(s"$outputBasePath/validation_differences.txt")

  Job.commit()
}

Expected Output

Case 1: Columns Exist

If all required columns exist and there are differences:

Country ISO Mapping Validation Results:
Source to Target Differences: 3 rows found 101, 202, 303
Target to Source Differences: 2 rows found 404, 505

Case 2: Missing Columns

If required columns are missing:

Validation Skipped: TRANSACTION_COUNTRY_ISO_CODE column not found in transactionSource.

Advantages of this Design

	1.	Dynamic Column Handling:
	•	The transformation is reusable and can handle dynamic mapping requirements.
	2.	Error-Resilient:
	•	Gracefully handles missing columns by skipping validation and logging an appropriate message.
	3.	AWS Glue Compatibility:
	•	Fully compatible with Spark and AWS Glue ETL jobs.

Let me know if you need further adjustments!





