import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

def applyDirectColumnValidation(
    sourceCols: String,
    targetCols: String,
    transactionSource: DataFrame,
    caseClass: DataFrame
)(implicit spark: SparkSession): List[String] = {
  import spark.implicits._

  // Split and align column names
  val sourceColumns = sourceCols.split(",").map(_.trim).toSeq
  val targetColumns = targetCols.split(",").map(_.trim).toSeq

  // Check for column count equality
  if (sourceColumns.length != targetColumns.length) {
    throw new IllegalArgumentException("Source and target column lists must have the same length.")
  }

  // Check that each column exists in the respective DataFrames, filtering out any missing columns
  val validSourceColumns = sourceColumns.filter(transactionSource.columns.contains)
  val validTargetColumns = targetColumns.filter(caseClass.columns.contains)

  // Report any missing columns
  val missingSourceColumns = sourceColumns.diff(validSourceColumns)
  val missingTargetColumns = targetColumns.diff(validTargetColumns)

  missingSourceColumns.foreach(col => println(s"Missing column in source: $col"))
  missingTargetColumns.foreach(col => println(s"Missing column in target: $col"))

  // Align columns and cast to String type for consistent comparison
  val sourceSelected = transactionSource.select(validSourceColumns.zipWithIndex.map {
    case (colName, i) => col(colName).cast("string").as(s"col_$i")
  }: _*)

  val targetSelected = caseClass.select(validTargetColumns.zipWithIndex.map {
    case (colName, i) => col(colName).cast("string").as(s"col_$i")
  }: _*)

  // Intersect the DataFrames to find matching rows
  val matchingData = sourceSelected.intersect(targetSelected)

  // Collect transaction IDs from the first column (assumed as "col_0" after renaming)
  matchingData.select("col_0").as[String].collect().toList
}