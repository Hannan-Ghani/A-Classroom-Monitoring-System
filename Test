import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession

object GlueApp {
  def main(sysArgs: Array[String]): Unit = {
    // Initialize Spark and Glue contexts
    val sc: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(sc)
    val spark: SparkSession = glueContext.getSparkSession

    // Parse the arguments from the Glue job
    val args = GlueArgParser.getResolvedOptions(
      sysArgs, 
      Array("sourcePath", "targetPath", "outputPath", "JOB_NAME")
    )

    val sourcePath = args("sourcePath")
    val targetPath = args("targetPath")
    val outputPath = args("outputPath")
    
    // Initialize the Glue job
    Job.init(args("JOB_NAME"), glueContext)

    // Load the source and target datasets dynamically
    val source = spark.read.parquet(sourcePath)
    val target = spark.read.parquet(targetPath)

    // Dynamically find common columns between source and target
    val commonColumns = source.columns.toSet.intersect(target.columns.toSet).toSeq
    println(s"Common Columns: ${commonColumns.mkString(", ")}")

    // If no common columns are found, log and exit the job
    if (commonColumns.isEmpty) {
      println("No common columns found between source and target datasets. Exiting job.")
      return
    }

    // Select only common columns from both datasets
    val sourceCommon = source.select(commonColumns.map(col): _*)
    val targetCommon = target.select(commonColumns.map(col): _*)

    // Compare rows by using `exceptAll`, which shows differences
    val differences = sourceCommon.exceptAll(targetCommon)

    // Check row count to ensure output will be written
    val diffCount = differences.count()
    println(s"Number of differences found: $diffCount")

    // Write the differences to the output path, or write an empty file if no differences
    if (diffCount > 0) {
      println(s"Writing differences to: $outputPath")
      differences.write.mode("overwrite").parquet(outputPath)
    } else {
      println(s"No differences found. Writing an empty file with schema to: $outputPath")
      val emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD[org.apache.spark.sql.Row], sourceCommon.schema)
      emptyDF.write.mode("overwrite").parquet(outputPath)
    }

    // Commit the job once completed
    Job.commit()
  }
}