import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SparkSession, Row}
import org.apache.spark.sql.functions._
import org.apache.log4j.Logger
import com.amazonaws.services.glue.util.GlueArgParser

object CleanseCaseClass {

  def main(sysArgs: Array[String]): Unit = {
    val sparkSession: SparkSession = SparkSession.builder.getOrCreate()
    val glueContext: GlueContext = new GlueContext(sparkSession.sparkContext)
    import sparkSession.implicits._

    val logger: Logger = Logger.getLogger("CleanseCaseClassLogger")

    // Get Glue job parameters
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3PathConfig", "s3ValidationConfig").toArray)
    val s3PathConfig = args("s3PathConfig")
    val s3ValidationConfig = args("s3ValidationConfig")

    // Read configuration files
    val pathConfigMap = parseConfigFile(s3PathConfig, sparkSession)
    val validationConfigMap = parseValidationConfig(s3ValidationConfig, sparkSession)

    // Extract paths
    val inputSourcePath = pathConfigMap("inputSourcePath")
    val inputTargetPath = pathConfigMap("inputTargetPath")
    val outputBasePath = pathConfigMap("outputBasePath")

    // Load data
    val transactionSource: DataFrame = sparkSession.read.parquet(inputSourcePath)
    val caseClass: DataFrame = sparkSession.read.parquet(inputTargetPath)

    // Initialize validation results
    var validationResults = Seq[String]()

    // Null Validation
    validationResults = validationResults ++ validateNullColumns(caseClass, validationConfigMap.getOrElse("null_validation", Seq()))

    // Direct Column Validation
    validationResults = validationResults ++ validateDirectColumns(transactionSource, caseClass, validationConfigMap)

    // Narrative Validation
    validationResults = validationResults ++ validateNarrativeColumns(transactionSource, caseClass, validationConfigMap)

    // Convert results to DataFrame and write to S3
    val resultsDF = validationResults.toDF("validation_result")
    resultsDF.write.mode("overwrite").text(s"$outputBasePath/validation_differences.txt")

    Job.commit()
  }

  def parseConfigFile(configPath: String, sparkSession: SparkSession): Map[String, String] = {
    import sparkSession.implicits._
    sparkSession.read.text(configPath).as[String].collect().flatMap { line =>
      val parts = line.split("=")
      if (parts.length == 2) {
        Some(parts(0).trim -> parts(1).trim)
      } else {
        println(s"Warning: Invalid config line - $line")
        None
      }
    }.toMap
  }

  def parseValidationConfig(configPath: String, sparkSession: SparkSession): Map[String, Seq[String]] = {
    import sparkSession.implicits._
    var currentSection = ""
    var validationConfig = Map[String, Seq[String]]()

    sparkSession.read.text(configPath).as[String].collect().foreach { line =>
      val trimmedLine = line.trim
      if (trimmedLine.startsWith("[") && trimmedLine.endsWith("]")) {
        currentSection = trimmedLine.substring(1, trimmedLine.length - 1)
        validationConfig += (currentSection -> Seq())
      } else if (trimmedLine.nonEmpty && currentSection.nonEmpty) {
        val columns = trimmedLine.split(",").map(_.trim).toSeq
        validationConfig += (currentSection -> columns)
      }
    }
    validationConfig
  }

  def validateNullColumns(df: DataFrame, nullColumns: Seq[String]): Seq[String] = {
    import df.sparkSession.implicits._
    val existingNullColumns = filterExistingColumns(df, nullColumns)
    if (existingNullColumns.nonEmpty) {
      val nonNullData = df.filter(existingNullColumns.map(col(_).isNotNull).reduce(_ || _))
      val nonNullTransactionIds = nonNullData.select($"transactionId").as[String].collect()
      Seq("Null Validation Differences:", "Non-null values found for columns expected to be null in rows: " + nonNullTransactionIds.mkString(", "))
    } else {
      Seq("Null Validation Differences: No relevant columns found for null validation.")
    }
  }

  def validateDirectColumns(sourceDF: DataFrame, targetDF: DataFrame, configMap: Map[String, Seq[String]]): Seq[String] = {
    import sourceDF.sparkSession.implicits._
    val sourceCols = configMap.getOrElse("direct_column_validation_source", Seq())
    val targetCols = configMap.getOrElse("direct_column_validation_target", Seq())

    val (alignedSourceCols, alignedTargetCols) = filterExistingColumnPairs(sourceCols, targetCols, sourceDF, targetDF)

    if (alignedSourceCols.nonEmpty && alignedTargetCols.nonEmpty) {
      val sourceData = sourceDF.select(
        col("TRANSACTION_ID") +: alignedSourceCols.zipWithIndex.map { case (column, i) => col(column).as(s"col_$i") }: _*
      )
      val targetData = targetDF.select(
        col("transactionId") +: alignedTargetCols.zipWithIndex.map { case (column, i) => col(column).as(s"col_$i") }: _*
      )

      val sourceToTargetDiff = sourceData.except(targetData)
      val targetToSourceDiff = targetData.except(sourceData)

      val sourceToTargetIds = sourceToTargetDiff.select($"TRANSACTION_ID").as[String].collect()
      val targetToSourceIds = targetToSourceDiff.select($"transactionId").as[String].collect()

      Seq(
        "Direct Column Validation Differences:",
        "Source to Target Differences: " + sourceToTargetIds.mkString(", "),
        "Target to Source Differences: " + targetToSourceIds.mkString(", ")
      )
    } else {
      Seq("Direct Column Validation Differences: No relevant columns found for direct validation.")
    }
  }

  def validateNarrativeColumns(sourceDF: DataFrame, targetDF: DataFrame, configMap: Map[String, Seq[String]]): Seq[String] = {
    import sourceDF.sparkSession.implicits._
    val sourceNarrativeCol = configMap.getOrElse("narrative_validation_source", Seq()).headOption.getOrElse("")
    val targetNarrativeCol = configMap.getOrElse("narrative_validation_target", Seq()).headOption.getOrElse("")

    val cleanedSourceNarrative = sourceDF.withColumn("cleaned_narrative", when(trim(col(sourceNarrativeCol)) === "" || trim(col(sourceNarrativeCol)) === "*", null)
      .otherwise(col(sourceNarrativeCol))
    ).select($"TRANSACTION_ID", $"cleaned_narrative")

    val targetNarrative = targetDF.select($"transactionId", col(targetNarrativeCol).as("cleaned_narrative"))

    val narrativeSourceToTargetDiff = cleanedSourceNarrative.except(targetNarrative)
    val narrativeTargetToSourceDiff = targetNarrative.except(cleanedSourceNarrative)

    val narrativeSourceToTargetIds = narrativeSourceToTargetDiff.select($"TRANSACTION_ID").as[String].collect()
    val narrativeTargetToSourceIds = narrativeTargetToSourceDiff.select($"transactionId").as[String].collect()

    Seq(
      "Narrative Validation Differences:",
      "Source to Target Differences: " + narrativeSourceToTargetIds.mkString(", "),
      "Target to Source Differences: " + narrativeTargetToSourceIds.mkString(", ")
    )
  }

  def filterExistingColumns(df: DataFrame, columns: Seq[String]): Seq[String] = {
    columns.filter(df.columns.contains)
  }

  def filterExistingColumnPairs(sourceColumns: Seq[String], targetColumns: Seq[String], sourceDF: DataFrame, targetDF: DataFrame): (Seq[String], Seq[String]) = {
    val sourceExists = sourceColumns.filter(sourceDF.columns.contains)
    val targetExists = targetColumns.filter(targetDF.columns.contains)
    (sourceExists, targetExists)
  }
}