import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.SparkContext
import scala.collection.JavaConverters._

object GlueApp {
  def main(sysArgs: Array[String]): Unit = {
    val sc: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(sc)
    val spark: SparkSession = glueContext.getSparkSession

    // Parse job parameters
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("sourcePath", "targetPath", "outputPath", "JOB_NAME").toArray)
    val sourcePath = args("sourcePath")
    val targetPath = args("targetPath")
    val outputPath = args("outputPath")

    // Read the source and target datasets
    val sourceDF: DataFrame = spark.read.parquet(sourcePath)
    val targetDF: DataFrame = spark.read.parquet(targetPath)

    // Dynamically select all columns for comparison
    val sourceColumns = sourceDF.columns.toSet
    val targetColumns = targetDF.columns.toSet

    // Check if both DataFrames have the same columns
    if (sourceColumns != targetColumns) {
      println(s"Source and target datasets have different columns: \nSource: $sourceColumns\nTarget: $targetColumns")
      System.exit(1)  // Exit if columns don't match
    }

    // Perform the comparison by finding mismatches
    val mismatchedRecords = sourceDF.except(targetDF).union(targetDF.except(sourceDF))

    // Output the mismatches
    if (!mismatchedRecords.isEmpty) {
      mismatchedRecords.show() // Show mismatched records
      // Save the mismatched records to the output path
      mismatchedRecords.write.mode("overwrite").parquet(outputPath)
    } else {
      println("No mismatches found. The datasets are identical.")
    }

    println("Glue job completed successfully.")
  }
}