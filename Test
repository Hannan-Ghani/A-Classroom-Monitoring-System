import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.sql.functions._
import org.apache.log4j.Logger

object CleanseCaseClass {

  def main(sysArgs: Array[String]): Unit = {
    // Initialize Glue and Spark context
    val spark: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(spark)
    val sparkSession: SparkSession = glueContext.getSparkSession
    import sparkSession.implicits._

    // Logger setup
    val logger: Logger = Logger.getLogger("CleanseCaseClassLogger")

    // Get the S3 config path from Glue job parameters
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3ConfigPath").toArray)
    val s3ConfigPath = args("s3ConfigPath")

    // Read the configuration file from S3
    val configDF = sparkSession.read.text(s3ConfigPath)

    // Convert the config DataFrame into a Map of key-value pairs
    val configMap = configDF.collect().map { row =>
      val split = row.getString(0).split("=")
      (split(0), split(1))
    }.toMap

    // Extract input and output paths from the config file
    val inputSourcePath = configMap("inputSourcePath")
    val inputTargetPath = configMap("inputTargetPath")
    val outputBasePath = configMap("outputCombinedExceptPath") // Base path for output files

    // Helper function to filter out missing columns and remove both source and target column pairs
    def getExistingColumnPairs(sourceColumns: Seq[String], targetColumns: Seq[String], sourceDF: DataFrame, targetDF: DataFrame): (Seq[String], Seq[String]) = {
      val sourceColumnsSet = sourceDF.columns.toSet
      val targetColumnsSet = targetDF.columns.toSet
      var removedColumns: Seq[(String, String)] = Seq()

      val filteredSourceColumns = sourceColumns.zip(targetColumns).flatMap {
        case (sourceCol, targetCol) =>
          if (sourceColumnsSet.contains(sourceCol) && targetColumnsSet.contains(targetCol)) {
            Some((sourceCol, targetCol))
          } else {
            removedColumns = removedColumns :+ (sourceCol, targetCol)
            None
          }
      }

      // Log the removed column pairs
      if (removedColumns.nonEmpty) {
        logger.warn(s"Removed column pairs due to missing columns: ${removedColumns.mkString(", ")}")
      }

      // Return the filtered source and target column pairs as two separate lists
      (filteredSourceColumns.map(_._1), filteredSourceColumns.map(_._2))
    }

    // =======================
    // PART 1: Read Source and Target Data
    // =======================

    // Read the transaction source Parquet file from the inputSourcePath in the config file
    val transactionSource: DataFrame = sparkSession.read.parquet(inputSourcePath)
      .filter($"transaction_date" > "2023-01-01" && $"transaction_date" < "2023-01-31")

    // Read the case class DataFrame from the inputTargetPath in the config file
    val caseClass: DataFrame = sparkSession.read.parquet(inputTargetPath)

    // =======================
    // PART 2: Narrative Validation using `exceptAll()`
    // =======================

    // Clean the "NARRATIVE" column in the source by replacing empty strings or asterisks with null
    val source_Narrative = transactionSource.withColumn(
      "NARRATIVE", 
      when(trim($"NARRATIVE") === "", null)
        .when(trim($"NARRATIVE") === "*", null)
        .otherwise($"NARRATIVE")
    ).select($"TRANSACTION_ID", $"NARRATIVE")

    // Select the corresponding narrative columns from the target
    val target_Narrative = caseClass.select($"transactionid", $"narrative")

    // Perform source-to-target validation using exceptAll
    val source_to_target_narrative_diff = source_Narrative.exceptAll(target_Narrative)
    // Perform target-to-source validation using exceptAll
    val target_to_source_narrative_diff = target_Narrative.exceptAll(source_Narrative)

    // Write separate Parquet files for source-to-target and target-to-source
    val narrative_output_path_source_to_target = s"${outputBasePath}/narrative_validation/source_to_target/"
    val narrative_output_path_target_to_source = s"${outputBasePath}/narrative_validation/target_to_source/"

    source_to_target_narrative_diff.write.mode("overwrite").parquet(narrative_output_path_source_to_target)
    target_to_source_narrative_diff.write.mode("overwrite").parquet(narrative_output_path_target_to_source)

    // =======================
    // PART 3: Null Value Validation (Target Only)
    // =======================

    // List of 41 columns for null validation (replace with actual column names)
    val targetNullColumns = Seq(
      "aBic", "aIban", "aBank", "aBankCountry", "aBankCountryIso3", "aAddress", "aParsedAddress", "aName", "bBranchUniqueId",
      "bBic", "bIban", "bBank", "bBankCountry", "bBankCountryIso3", "bAddress", "bParsedAddress", "bName", 
      "aCounterpartyBankBic", "CounterpartyBankName", "aCounterpartyBankCountry", "aCounterpartyBankCountryIso3",
      "bCounterpartyBankBic", "bCounterpartyBankName", "bCounterpartyBankCountry", "bCounterpartyBankCountryIso3", 
      "interBankBic", "interBankName", "interBankCountry", "interBankCountryIso3", "reference", "additionalReference", 
      "format", "crossborder", "charges", "sanctionsStatus", "paymentsStatus", "aAccountNumberCleansed", "amountUsd",
      "aAccountOpenDate", "bAccountOpenDate", "aggregatedTransactionDirectionAToB"
    )

    // Filter out missing columns from the target for null validation
    val existingTargetNullColumns = targetNullColumns.filter(col => caseClass.columns.contains(col))
    if (existingTargetNullColumns.size < targetNullColumns.size) {
      logger.warn(s"Some columns were missing in target null validation: ${targetNullColumns.diff(existingTargetNullColumns)}")
    }

    // Select and use distinct() for target null validation columns
    val targetNullData = caseClass.select(existingTargetNullColumns.map(col): _*).distinct()

    // Write null value validation to separate Parquet file
    val null_validation_output_path = s"${outputBasePath}/null_validation/"
    targetNullData.write.mode("overwrite").parquet(null_validation_output_path)

    // =======================
    // PART 4: Direct Column Validation using `exceptAll()`
    // =======================

    // Define source and target columns for direct validation (with pairs)
    val sourceDirectColumns = Seq(
      "TRANSACTION_ID", "TRANSACTION_TYPE", "TRANSACTION_TYPE_DESC", "SHORTNARRATIVE",
      "TRANSACTION_CODE", "TRANSACTION_CODE_DESC", "SOURCE_TRANSACTION_ID",
      "ORIGINAL_CURRENCY_CODE", "TRANSACTION_COUNTRY", "TRANSACTION_LOCATION", "screening_system"
    )

    val targetDirectColumns = Seq(
      "transactionId", "transactionType", "transactionTypeDescription", "narrativeShort",
      "transactionCode", "transactionCodeDescription", "sourceTransactionId",
      "currencyLocal", "transactionCountry", "transactionLocation", "sourceSystem"
    )

    // Filter out missing column pairs for direct validation
    val (existingSourceDirectColumns, existingTargetDirectColumns) = getExistingColumnPairs(sourceDirectColumns, targetDirectColumns, transactionSource, caseClass)

    // Select and filter columns for direct validation
    val sourceDirectData = transactionSource.select(existingSourceDirectColumns.map(col): _*)
    val targetDirectData = caseClass.select(existingTargetDirectColumns.map(col): _*)

    // Perform source-to-target validation using exceptAll
    val direct_source_to_target_diff = sourceDirectData.exceptAll(targetDirectData)
    // Perform target-to-source validation using exceptAll
    val direct_target_to_source_diff = targetDirectData.exceptAll(sourceDirectData)

    // Write separate Parquet files for source-to-target and target-to-source
    val direct_output_path_source_to_target = s"${outputBasePath}/direct_column_validation/source_to_target/"
    val direct_output_path_target_to_source = s"${outputBasePath}/direct_column_validation/target_to_source/"

    direct_source_to_target_diff.write.mode("overwrite").parquet(direct_output_path_source_to_target)
    direct_target_to_source_diff.write.mode("overwrite").parquet(direct_output_path_target_to_source)

    // Commit the Glue job to mark it as successfully completed
    Job.commit()
  }
}