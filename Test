import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, regexp_replace, when

# Function to initialize Spark and Glue contexts
def initialize_context(args):
    sc = SparkContext()
    glue_context = GlueContext(sc)
    spark = glue_context.spark_session
    job = Job(glue_context)
    job.init(args['JOB_NAME'], args)
    return spark, job

# Function to parse paths from a text file
def read_input_paths(file_path: str) -> dict:
    with open(file_path, 'r') as file:
        return {line.split('=')[0].strip(): line.split('=')[1].strip() for line in file if '=' in line}

# Function to parse column configurations from a text file
def read_column_config(file_path: str) -> dict:
    config = {}
    with open(file_path, 'r') as file:
        section = None
        for line in file:
            line = line.strip()
            if line.startswith("[") and line.endswith("]"):  # Section header
                section = line[1:-1]
                config[section] = []
            elif section and line:
                config[section].append(line)
    return config

# Function to read and process customer data
def process_customer_data(spark: DataFrame, customer_path: str, column_config: dict) -> DataFrame:
    customer_key = column_config['customer'][0]
    return spark.read.parquet(customer_path)\
        .filter("""
            NUM_OF_ACCOUNT >= 0 AND 
            (SOURCE_COUNTRY <> 'ZA' OR SOURCE_COUNTRY IS NULL) AND 
            customer_key IS NOT NULL AND 
            customer_key NOT IN ('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')
        """)\
        .withColumn('customerUniqueId', regexp_replace(col(customer_key), '^-', ''))\
        .cache()

# Function to read product data
def read_product_data(spark: DataFrame, fortent_path: str, core_path: str) -> DataFrame:
    prod_fortent = spark.read.parquet(fortent_path)
    prod_core = spark.read.parquet(core_path)
    return prod_core.unionByName(prod_fortent).cache()

# Function to join customer and product data
def join_customer_product(a_cus: DataFrame, a_prod: DataFrame) -> DataFrame:
    return a_cus.join(a_prod, a_cus.customerUniqueId == a_prod.customerUniqueId, how='inner')\
        .dropDuplicates().cache()

# Scenario 1: Compare account numbers
def validate_account_numbers(spark: DataFrame, column_config: dict) -> DataFrame:
    customer_col = column_config['customer'][0]
    account_col = column_config['account'][0]
    return spark.sql(f"""
        SELECT 
            a.transactionId
        FROM prod_qtx a
        LEFT JOIN Qxta_targt b
        ON a.{customer_col} = b.{customer_col}
        WHERE a.{account_col} != b.{account_col}
    """)

# Scenario 2: Compare sort codes
def validate_sort_codes(spark: DataFrame, column_config: dict) -> DataFrame:
    customer_col = column_config['customer'][0]
    sortcode_col = column_config['sortcode'][0]
    return spark.sql(f"""
        SELECT 
            a.transactionId
        FROM prod_qtx a
        LEFT JOIN Qxta_targt b
        ON a.{customer_col} = b.{customer_col}
        WHERE a.{sortcode_col} != b.{sortcode_col}
    """)

# Scenario 3: Differences Validation
def validate_data_differences(prod_qtx: DataFrame, target_table: str, column_config: dict) -> DataFrame:
    columns = column_config['validate']
    src_fields1 = prod_qtx.select("transactionId")
    targ_fields1 = spark.table(target_table).select("transactionId")
    
    source_to_target_diff = src_fields1.exceptAll(targ_fields1)
    target_to_source_diff = targ_fields1.exceptAll(src_fields1)
    
    # Combine results
    return source_to_target_diff.unionByName(target_to_source_diff)

# Aggregate results and write to a text file
def aggregate_and_write_results(validation_results: list, output_path: str):
    with open(output_path, 'w') as file:
        for idx, (scenario_name, result) in enumerate(validation_results):
            transaction_ids = result.select("transactionId").distinct()
            count = transaction_ids.count()
            
            file.write(f"Validation Scenario {idx + 1}: {scenario_name}\n")
            file.write(f"Total Differences: {count}\n")
            transaction_ids_list = transaction_ids.collect()
            for row in transaction_ids_list:
                file.write(f"{row['transactionId']}\n")
            file.write("\n")

# Main entry point
if __name__ == "__main__":
    args = getResolvedOptions(
        sys.argv, 
        ['JOB_NAME', 'INPUT_PATH_FILE', 'COLUMN_CONFIG_FILE', 'OUTPUT_PATH']
    )
    
    # Initialize contexts and job
    spark, job = initialize_context(args)
    
    # Read input paths and column configurations
    input_paths = read_input_paths(args['INPUT_PATH_FILE'])
    column_config = read_column_config(args['COLUMN_CONFIG_FILE'])
    
    # Process data
    a_cus = process_customer_data(spark, input_paths['CUSTOMER_PATH'], column_config)
    a_prod = read_product_data(spark, input_paths['PRODUCT_FORTENT_PATH'], input_paths['PRODUCT_CORE_PATH'])
    
    # Join customer and product data
    prod_qtx = join_customer_product(a_cus, a_prod)
    prod_qtx.createOrReplaceTempView('prod_qtx')
    
    # Execute validation scenarios
    scenario_1_result = validate_account_numbers(spark, column_config)
    scenario_2_result = validate_sort_codes(spark, column_config)
    scenario_3_result = validate_data_differences(prod_qtx, "Qxta_targt", column_config)
    
    # Aggregate and write results
    aggregate_and_write_results(
        [
            ("Account Number Differences", scenario_1_result),
            ("Sort Code Differences", scenario_2_result),
            ("Data Differences", scenario_3_result)
        ],
        args['OUTPUT_PATH']
    )
    
    # Commit the Glue job
    job.commit()