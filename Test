from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_replace, lit, when, explode

# Initialize Spark session
spark = SparkSession.builder.appName("CustomerProductDataProcessing").getOrCreate()

# Read the customer data
a_cus = (
    spark.read.parquet("s3://136731789529-fis-raw-data/customer/20240129/raw/parquet/*.parquet")
    .filter(
        "NUM_OF_ACCOUNT >= 0 AND "
        "(SOURCE_COUNTRY <> 'ZA' OR SOURCE_COUNTRY IS NULL) AND "
        "customer_key IS NOT NULL AND "
        "customer_key NOT IN ('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')"
    )
    .withColumn(
        "customerUniqueId", 
        regexp_replace(col("customer_key"), "^.*-", "")  # Clean up 'customer_key'
    )
)

a_cus.createOrReplaceTempView("a_cus")
a_cus.cache()

# Read the product data
a_prod = (
    spark.read.parquet("s3://folder2/parquet/")
    .filter(
        (col("account_key").isNotNull() | col("customer_key").isNotNull()) &
        ~(
            (col("screening_system") == "YMUK2") &
            (
                (~col("customer_role").isin("CML - COMP PARENT", "PRIMARY")) |
                col("low_account_type").isin(
                    "UKCMLPHVI", "UKCMLPV100", "UKCMLPV5K", "UKCMLPV500", 
                    "UKCMLPV10K", "UKCMLPV1K", "UKCMLPVUNK", "UKCMLPBHI", 
                    "UKCMLPB100", "UKCMLPB5K", "UKCMLPB500", "UKCMLPB1K", "UKCMLPBUNK"
                )
            )
        )
    )
    .withColumn(
        "customerUniqueId", 
        regexp_replace(col("customer_key"), "^.*-", "")  # Clean up 'customer_key'
    )
)

a_prod.createOrReplaceTempView("a_prod")
a_prod.cache()

# Join customer and product data
prod_qtx = (
    a_cus.join(a_prod, a_cus.customerUniqueId == a_prod.customerUniqueId, how="inner")
    .select(a_prod["*"])  # Select all columns from 'a_prod'
)

prod_qtx.createOrReplaceTempView("prod_qtx")
prod_qtx.cache()

# Process 'prod_qtx' fields
src_fields = spark.sql("""
    SELECT 
        customerUniqueId,
        account_risk_code_desc,
        name AS accountName,
        SUBSTRING(account_id, 0, 5) AS accountIdPrefix,
        account_no AS AccountNumber,
        account_no AS cleansedAccountNumber,
        account_key AS accountKey,
        company_id AS companyId,
        country_code AS country,
        currency_code AS currencyCode,
        CASE WHEN financial_institution = 'N' THEN 'false' ELSE 'true' END AS financialInstitution,
        CASE WHEN joint_account = 'N' THEN 'false' ELSE 'true' END AS jointAccount,
        update_tms AS lastUpdateData,
        line_of_business_desc AS lineOfBusiness,
        maturity_date AS maturityDate,
        CASE WHEN non_operating_entity = 'N' THEN 'false' ELSE 'true' END AS nonOperatingEntity,
        CASE WHEN numbered_account = 'N' THEN 'false' ELSE 'true' END AS numberedAccount,
        product_id AS productId,
        region_id AS regionId,
        risk_code AS riskCode,
        relman_id AS rmId,
        relman_name AS rmName,
        CASE WHEN sensitive_industry = 'N' THEN 'false' ELSE 'true' END AS sensitiveIndustry,
        sortcode AS sortcode,
        sortcode AS cleansedSortCode,
        source_sys_code AS sourceSystem,
        high_account_type AS highAccountType,
        high_account_type_desc AS highAccountTypeDescription,
        low_account_type AS lowAccountType,
        low_account_type_desc AS lowAccountTypeDescription,
        externalAccountType,
        industry_code_desc AS industryCodeDescription
    FROM prod_qtx
""")

src_fields.createOrReplaceTempView("src_fields")
src_fields.cache()

# Read and explode the target Parquet data
Qxta_targt = (
    spark.read.parquet("s3://folder3/parquet/")
    .select(
        "customerUniqueId", 
        explode(col("account")).alias("account")  # Explode the 'account' array
    )
    .select(
        "customerUniqueId", 
        "account.*"  # Expand all fields from the exploded 'account' struct
    )
)

Qxta_targt.createOrReplaceTempView("Qxta_targt")
Qxta_targt.cache()

# Select required fields from 'Qxta_targt'
selected_Qxta_targt = spark.sql("""
    SELECT 
        customerUniqueId, 
        accountRiskCodeDescription, 
        accountName, 
        accountIdPrefix, 
        accountNumber, 
        cleansedAccountNumber, 
        accountKey, 
        companyId, 
        country, 
        currencyCode, 
        CAST(financialInstitution AS STRING) AS financialInstitution, 
        CAST(jointAccount AS STRING) AS jointAccount, 
        lastUpdatedDate, 
        lineOfBusiness, 
        lineOfBusinessDescription, 
        maturityDate, 
        CAST(nonOperatingEntity AS STRING) AS nonOperatingEntity, 
        CAST(numberedAccount AS STRING) AS numberedAccount, 
        productId, 
        regionId, 
        riskCode, 
        rmId, 
        rmName, 
        CAST(sensitiveIndustry AS STRING) AS sensitiveIndustry, 
        sortcode, 
        cleansedSortCode, 
        sourceSystem, 
        highAccountType, 
        highAccountTypeDescription, 
        lowAccountType, 
        lowAccountTypeDescription, 
        externalAccountType, 
        industryCodeDescription 
    FROM Qxta_targt
""")

selected_Qxta_targt.createOrReplaceTempView("selected_Qxta_targt")
selected_Qxta_targt.cache()

# Scenario 1 Fields: src
src_fields1 = (
    spark.sql("""
        SELECT 
            customerUniqueId, 
            account_risk_code_desc, 
            accountName, 
            accountIdPrefix 
        FROM src_fields
    """)
    .dropDuplicates()
)

src_fields1.createOrReplaceTempView("src_fields1")
src_fields1.cache()

# Scenario 2 Fields: trgt
targ_fields1 = (
    spark.sql("""
        SELECT 
            customerUniqueId, 
            accountRiskCodeDescription AS account_risk_code_desc, 
            accountName, 
            accountIdPrefix 
        FROM selected_Qxta_targt
    """)
    .dropDuplicates()
)

targ_fields1.createOrReplaceTempView("targ_fields1")
targ_fields1.cache()

# Differences Scenario 1: Compare all the data from src to target
print("Differences: src_fields1 vs targ_fields1")
src_fields1.exceptAll(targ_fields1).show(10, False)

print("Differences: targ_fields1 vs src_fields1")
targ_fields1.exceptAll(src_fields1).show(10, False)