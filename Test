The error indicating that s3://Restofpath/Validation_differences.txt has “no such file or directory” means that the specified S3 path does not exist or isn’t accessible for writing.

In AWS Glue, writing to an S3 path requires:
	1.	Correct Permissions: Ensure that the IAM role for the Glue job has write permissions to the specified S3 bucket and path.
	2.	Correct S3 Path Format: S3 paths should follow the format s3://bucket-name/path/to/file.

Steps to Resolve

	1.	Ensure Permissions:
	•	Verify that the IAM role assigned to the Glue job has s3:PutObject permission for the target S3 bucket and path.
	•	You can check and adjust this in the IAM Console.
	2.	Use Spark’s S3 Write Function:
	•	Since PrintWriter is intended for writing to local files, it will not work directly with S3 paths. Instead, use Spark’s DataFrame.write to save data to S3.
	•	I’ll modify the code to collect the output as a DataFrame and then write it to S3.

Code Modification

Here’s how to modify the code to accumulate validation results and write them to S3 using Spark:
	1.	Accumulate Validation Results in a List: Store each validation result in a Seq[String].
	2.	Convert to DataFrame and Write to S3: Create a DataFrame from the results and use Spark’s write function to save it to S3.

Here’s the modified code for writing validation results to S3:

import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SparkSession, Row}
import org.apache.spark.sql.functions._
import org.apache.log4j.Logger
import com.amazonaws.services.glue.util.GlueArgParser

object CleanseCaseClass {

  def main(sysArgs: Array[String]): Unit = {
    val sparkSession: SparkSession = SparkSession.builder.getOrCreate()
    val glueContext: GlueContext = new GlueContext(sparkSession.sparkContext)
    import sparkSession.implicits._

    val logger: Logger = Logger.getLogger("CleanseCaseClassLogger")

    // Get Glue job parameters
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3PathConfig", "s3ValidationConfig").toArray)
    val s3PathConfig = args("s3PathConfig")
    val s3ValidationConfig = args("s3ValidationConfig")

    // Read configuration files
    val pathConfigMap = parseConfigFile(s3PathConfig, sparkSession)
    val validationConfigMap = parseValidationConfig(s3ValidationConfig, sparkSession)

    // Extract paths
    val inputSourcePath = pathConfigMap("inputSourcePath")
    val inputTargetPath = pathConfigMap("inputTargetPath")
    val outputBasePath = pathConfigMap("outputBasePath")

    // Load data
    val transactionSource: DataFrame = sparkSession.read.parquet(inputSourcePath)
    val caseClass: DataFrame = sparkSession.read.parquet(inputTargetPath)

    // Initialize validation results
    var validationResults = Seq[String]()

    // Null Validation
    validationResults = validationResults ++ validateNullColumns(caseClass, validationConfigMap.getOrElse("null_validation", Seq()))

    // Direct Column Validation
    validationResults = validationResults ++ validateDirectColumns(transactionSource, caseClass, validationConfigMap)

    // Narrative Validation
    validationResults = validationResults ++ validateNarrativeColumns(transactionSource, caseClass, validationConfigMap)

    // Convert results to DataFrame and write to S3
    val resultsDF = validationResults.toDF("validation_result")
    resultsDF.write.mode("overwrite").text(s"$outputBasePath/validation_differences.txt")

    Job.commit()
  }

  def parseConfigFile(configPath: String, sparkSession: SparkSession): Map[String, String] = {
    import sparkSession.implicits._
    sparkSession.read.text(configPath).as[String].collect().flatMap { line =>
      val parts = line.split("=")
      if (parts.length == 2) {
        Some(parts(0).trim -> parts(1).trim)
      } else {
        println(s"Warning: Invalid config line - $line")
        None
      }
    }.toMap
  }

  def parseValidationConfig(configPath: String, sparkSession: SparkSession): Map[String, Seq[String]] = {
    import sparkSession.implicits._
    var currentSection = ""
    var validationConfig = Map[String, Seq[String]]()

    sparkSession.read.text(configPath).as[String].collect().foreach { line =>
      val trimmedLine = line.trim
      if (trimmedLine.startsWith("[") && trimmedLine.endsWith("]")) {
        currentSection = trimmedLine.substring(1, trimmedLine.length - 1)
        validationConfig += (currentSection -> Seq())
      } else if (trimmedLine.nonEmpty && currentSection.nonEmpty) {
        val columns = trimmedLine.split(",").map(_.trim).toSeq
        validationConfig += (currentSection -> columns)
      }
    }
    validationConfig
  }

  def validateNullColumns(df: DataFrame, nullColumns: Seq[String]): Seq[String] = {
    import df.sparkSession.implicits._
    val existingNullColumns = filterExistingColumns(df, nullColumns)
    if (existingNullColumns.nonEmpty) {
      val nonNullData = df.filter(existingNullColumns.map(col(_).isNotNull).reduce(_ || _))
      val nonNullTransactionIds = nonNullData.select($"transactionId").as[String].collect()
      Seq("Null Validation Differences:", "Non-null values found for columns expected to be null in rows: " + nonNullTransactionIds.mkString(", "))
    } else {
      Seq("Null Validation Differences: No relevant columns found for null validation.")
    }
  }

  def validateDirectColumns(sourceDF: DataFrame, targetDF: DataFrame, configMap: Map[String, Seq[String]]): Seq[String] = {
    import sourceDF.sparkSession.implicits._
    val sourceCols = configMap.getOrElse("direct_column_validation_source", Seq())
    val targetCols = configMap.getOrElse("direct_column_validation_target", Seq())

    val (alignedSourceCols, alignedTargetCols) = filterExistingColumnPairs(sourceCols, targetCols, sourceDF, targetDF)

    if (alignedSourceCols.nonEmpty && alignedTargetCols.nonEmpty) {
      val sourceData = sourceDF.select(
        col("TRANSACTION_ID") +: alignedSourceCols.zipWithIndex.map { case (column, i) => col(column).as(s"col_$i") }: _*
      )
      val targetData = targetDF.select(
        col("transactionId") +: alignedTargetCols.zipWithIndex.map { case (column, i) => col(column).as(s"col_$i") }: _*
      )

      val sourceToTargetDiff = sourceData.except(targetData)
      val targetToSourceDiff = targetData.except(sourceData)

      val sourceToTargetIds = sourceToTargetDiff.select($"TRANSACTION_ID").as[String].collect()
      val targetToSourceIds = targetToSourceDiff.select($"transactionId").as[String].collect()

      Seq(
        "Direct Column Validation Differences:",
        "Source to Target Differences: " + sourceToTargetIds.mkString(", "),
        "Target to Source Differences: " + targetToSourceIds.mkString(", ")
      )
    } else {
      Seq("Direct Column Validation Differences: No relevant columns found for direct validation.")
    }
  }

  def validateNarrativeColumns(sourceDF: DataFrame, targetDF: DataFrame, configMap: Map[String, Seq[String]]): Seq[String] = {
    import sourceDF.sparkSession.implicits._
    val sourceNarrativeCol = configMap.getOrElse("narrative_validation_source", Seq()).headOption.getOrElse("")
    val targetNarrativeCol = configMap.getOrElse("narrative_validation_target", Seq()).headOption.getOrElse("")

    val cleanedSourceNarrative = sourceDF.withColumn("cleaned_narrative", when(trim(col(sourceNarrativeCol)) === "" || trim(col(sourceNarrativeCol)) === "*", null)
      .otherwise(col(sourceNarrativeCol))
    ).select($"TRANSACTION_ID", $"cleaned_narrative")

    val targetNarrative = targetDF.select($"transactionId", col(targetNarrativeCol).as("cleaned_narrative"))

    val narrativeSourceToTargetDiff = cleanedSourceNarrative.except(targetNarrative)
    val narrativeTargetToSourceDiff = targetNarrative.except(cleanedSourceNarrative)

    val narrativeSourceToTargetIds = narrativeSourceToTargetDiff.select($"TRANSACTION_ID").as[String].collect()
    val narrativeTargetToSourceIds = narrativeTargetToSourceDiff.select($"transactionId").as[String].collect()

    Seq(
      "Narrative Validation Differences:",
      "Source to Target Differences: " + narrativeSourceToTargetIds.mkString(", "),
      "Target to Source Differences: " + narrativeTargetToSourceIds.mkString(", ")
    )
  }

  def filterExistingColumns(df: DataFrame, columns: Seq[String]): Seq[String] = {
    columns.filter(df.columns.contains)
  }

  def filterExistingColumnPairs(sourceColumns: Seq[String], targetColumns: Seq[String], sourceDF: DataFrame, targetDF: DataFrame): (Seq[String], Seq[String]) = {
    val sourceExists = sourceColumns.filter(sourceDF.columns.contains)
    val targetExists = targetColumns.filter(targetDF.columns.contains)
    (sourceExists, targetExists)
  }
}

Explanation of Changes

	•	Accumulate Results: Each validation method now returns a Seq[String] of results.
	•	Convert to DataFrame: The accumulated results are converted to a DataFrame.
	•	Write to S3: Using DataFrame.write.text to save directly to the S3 path specified.

This approach bypasses the need for PrintWriter and directly saves results to S3. Let me know if this resolves the issue!