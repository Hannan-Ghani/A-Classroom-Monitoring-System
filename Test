import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.sql.functions._
import org.apache.log4j.Logger

object CleanseCaseClass {

  def main(sysArgs: Array[String]): Unit = {
    // Initialize Glue and Spark context
    val spark: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(spark)
    val sparkSession: SparkSession = glueContext.getSparkSession
    import sparkSession.implicits._

    // Logger setup
    val logger: Logger = Logger.getLogger("CleanseCaseClassLogger")

    // Get the S3 config path from Glue job parameters
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3ConfigPath").toArray)
    val s3ConfigPath = args("s3ConfigPath")

    // Read the configuration file from S3
    val configDF = sparkSession.read.text(s3ConfigPath)

    // Convert the config DataFrame into a Map of key-value pairs
    val configMap = configDF.collect().map { row =>
      val split = row.getString(0).split("=")
      (split(0), split(1))
    }.toMap

    // Extract input and output paths from the config file
    val inputSourcePath = configMap("inputSourcePath")
    val inputTargetPath = configMap("inputTargetPath")
    val outputBasePath = configMap("outputCombinedExceptPath") // Base path for output files

    // =======================
    // PART 1: Read Source and Target Data
    // =======================

    // Read the transaction source Parquet file from the inputSourcePath in the config file
    val transactionSource: DataFrame = sparkSession.read.parquet(inputSourcePath)
      .filter($"transaction_date" > "2023-01-01" && $"transaction_date" < "2023-01-31")

    // Read the case class DataFrame from the inputTargetPath in the config file
    val caseClass: DataFrame = sparkSession.read.parquet(inputTargetPath)

    // =======================
    // PART 2: Narrative Validation using `exceptAll()`
    // =======================

    // Clean the "NARRATIVE" column in the source by replacing empty strings or asterisks with null
    val source_Narrative = transactionSource.withColumn(
      "NARRATIVE", 
      when(trim($"NARRATIVE") === "", null)
        .when(trim($"NARRATIVE") === "*", null)
        .otherwise($"NARRATIVE")
    ).select($"TRANSACTION_ID", $"NARRATIVE")

    // Select the corresponding narrative columns from the target
    val target_Narrative = caseClass.select($"transactionid", $"narrative")

    // Perform source-to-target validation using exceptAll
    val source_to_target_narrative_diff = source_Narrative.exceptAll(target_Narrative)

    // Perform target-to-source validation using exceptAll
    val target_to_source_narrative_diff = target_Narrative.exceptAll(source_Narrative)

    // Combine source and target columns side-by-side without using join
    val narrative_combined = source_to_target_narrative_diff.withColumnRenamed("NARRATIVE", "source_NARRATIVE")
      .withColumn("target_NARRATIVE", lit(null).cast("string"))
      .unionByName(target_to_source_narrative_diff.withColumnRenamed("narrative", "target_NARRATIVE")
      .withColumn("source_NARRATIVE", lit(null).cast("string")))

    // Write narrative validation to separate Parquet file
    val narrative_output_path = s"${outputBasePath}/narrative_validation/"
    narrative_combined.write.mode("overwrite").parquet(narrative_output_path)

    // =======================
    // PART 3: Null Value Validation (Target Only)
    // =======================
    
    // Select the 41 columns from target for null validation
    val targetNullColumns = Seq( /* Placeholder for 41 target columns */ )

    // Select and use distinct() for target null validation columns
    val targetNullData = caseClass.select(targetNullColumns.map(col): _*).distinct()

    // Write null value validation to separate Parquet file
    val null_validation_output_path = s"${outputBasePath}/null_validation/"
    targetNullData.write.mode("overwrite").parquet(null_validation_output_path)

    // =======================
    // PART 4: Direct Column Validation using `exceptAll()`
    // =======================

    // Define source and target columns for direct validation
    val sourceDirectColumns = Seq(
      "TRANSACTION_ID", "TRANSACTION_TYPE", "TRANSACTION_TYPE_DESC", "SHORTNARRATIVE",
      "TRANSACTION_CODE", "TRANSACTION_CODE_DESC", "SOURCE_TRANSACTION_ID",
      "ORIGINAL_CURRENCY_CODE", "TRANSACTION_COUNTRY", "TRANSACTION_LOCATION", "screening_system"
    )

    val targetDirectColumns = Seq(
      "transactionId", "transactionType", "transactionTypeDescription", "narrativeShort",
      "transactionCode", "transactionCodeDescription", "sourceTransactionId",
      "currencyLocal", "transactionCountry", "transactionLocation", "sourceSystem"
    )

    // Select and filter columns for direct validation
    val sourceDirectData = transactionSource.select(sourceDirectColumns.map(col): _*)
    val targetDirectData = caseClass.select(targetDirectColumns.map(col): _*)

    // Perform source-to-target validation using exceptAll
    val direct_source_to_target_diff = sourceDirectData.exceptAll(targetDirectData)

    // Perform target-to-source validation using exceptAll
    val direct_target_to_source_diff = targetDirectData.exceptAll(sourceDirectData)

    // Combine source and target columns side-by-side without using join
    val direct_combined = direct_source_to_target_diff.withColumnRenamed("TRANSACTION_ID", "source_TRANSACTION_ID")
      .withColumn("target_TRANSACTION_ID", lit(null).cast("string"))
      .unionByName(direct_target_to_source_diff.withColumnRenamed("transactionId", "target_TRANSACTION_ID")
      .withColumn("source_TRANSACTION_ID", lit(null).cast("string")))

    // Write direct column validation to separate Parquet file
    val direct_validation_output_path = s"${outputBasePath}/direct_column_validation/"
    direct_combined.write.mode("overwrite").parquet(direct_validation_output_path)

    // Commit the Glue job to mark it as successfully completed
    Job.commit()
  }
}