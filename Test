import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, regexp_replace, when, explode

# Initialize Glue job and Spark context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CUSTOMER_PATH', 'PRODUCT_FORTENT_PATH', 'PRODUCT_CORE_PATH', 'DOCUMENT_PATH', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Initialize logger
logger = glueContext.get_logger()

# Function to log and handle missing columns
def check_columns_and_log(df1, df2, columns, scenario_name):
    df1_columns = set(df1.columns)
    df2_columns = set(df2.columns)
    
    missing_in_df1 = set(columns) - df1_columns
    missing_in_df2 = set(columns) - df2_columns
    
    if missing_in_df1:
        logger.info(f"{scenario_name}: The following columns are missing in the source: {missing_in_df1}")
    
    if missing_in_df2:
        logger.info(f"{scenario_name}: The following columns are missing in the target: {missing_in_df2}")
    
    # Only use columns that exist in both DataFrames for comparison
    valid_columns = set(columns) & df1_columns & df2_columns
    
    return list(valid_columns)

# Read customer data
def read_customer_data():
    a_cus = spark.read.parquet(args['CUSTOMER_PATH'])\
        .filter("NUM_OF_ACCOUNT >= 0 and (SOURCE_COUNTRY <> 'ZA' Or SOURCE_COUNTRY IS NULL) and customer_key is not null and customer_key not in('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')")\
        .withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    a_cus.createOrReplaceTempView('a_cus')
    a_cus.cache()
    return a_cus

# Read product data
def read_product_data():
    Prod_fortent = spark.read.parquet(args['PRODUCT_FORTENT_PATH'])
    Prod_fortent.createOrReplaceTempView('Prod_fortent')
    Prod_fortent.cache()

    Prod_core = spark.read.parquet(args['PRODUCT_CORE_PATH'])
    Prod_core.createOrReplaceTempView('Prod_core')
    Prod_core.cache()

    # Union operation
    prodJoin = Prod_core.unionByName(Prod_fortent)
    prodJoin.createOrReplaceTempView('prodJoin')
    return prodJoin

# Insert the prod_qtx view creation here!
def create_prod_qtx(a_cus, prodJoin):
    # Ensure that customerUniqueId exists in prodJoin as well
    prodJoin = prodJoin.withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    
    # Perform an inner join between customer and product data to create prod_qtx
    prod_qtx = a_cus.join(prodJoin, 'customerUniqueId', how='inner')\
        .select(prodJoin['*'])\
        .dropDuplicates()
    
    prod_qtx.createOrReplaceTempView('prod_qtx')
    prod_qtx.cache()

# Create the views src_fields and targ_fields
def create_views():
    # Create src_fields from the product data (prod_qtx)
    src_fields = spark.sql("""
        SELECT customerUniqueId, account_risk_code, account_id, account_risk_code_desc
        FROM prod_qtx
    """)
    src_fields.createOrReplaceTempView('src_fields')

    # Create targ_fields from another relevant dataset
    targ_fields = spark.sql("""
        SELECT customerUniqueId, cleansedAccountNumber, accountNumber, accountKey, companyId
        FROM some_target_dataset
    """)
    targ_fields.createOrReplaceTempView('targ_fields')

# Scenario 1: Source to Target Comparison (Direct Columns)
def scenario_1():
    src_fields1 = spark.sql("""
        SELECT * 
        FROM src_fields
    """)
    targ_fields1 = spark.sql("""
        SELECT * 
        FROM targ_fields
    """)
    
    # Specify the columns you want to compare in Scenario 1
    columns = ['customerUniqueId', 'accountNumber', 'accountKey']  # Example columns

    # Check for missing columns and log them
    valid_columns = check_columns_and_log(src_fields1, targ_fields1, columns, "Scenario 1")

    # Only compare columns that are present in both DataFrames
    src_to_tgt_diff = src_fields1.select(valid_columns).exceptAll(targ_fields1.select(valid_columns))
    tgt_to_src_diff = targ_fields1.select(valid_columns).exceptAll(src_fields1.select(valid_columns))

    return src_to_tgt_diff.unionByName(tgt_to_src_diff)

# Scenario 2: Customer Data with Cleansed Account Number
def scenario_2():
    src_fields2 = spark.sql("""
        SELECT * 
        FROM src_fields
    """)
    targ_fields2 = spark.sql("""
        SELECT * 
        FROM targ_fields
    """)

    # Specify columns to compare
    columns = ['customerUniqueId', 'cleansedAccountNumber', 'accountNumber', 'accountKey']  # Example columns for scenario 2

    # Check for missing columns and log them
    valid_columns = check_columns_and_log(src_fields2, targ_fields2, columns, "Scenario 2")

    # Only compare columns that are present in both DataFrames
    src_to_tgt_diff = src_fields2.select(valid_columns).exceptAll(targ_fields2.select(valid_columns))
    tgt_to_src_diff = targ_fields2.select(valid_columns).exceptAll(src_fields2.select(valid_columns))

    return src_to_tgt_diff.unionByName(tgt_to_src_diff)

# Scenario 3: Narrative and Transaction Country Comparison
def scenario_3():
    src_fields3 = spark.sql("""
        SELECT * 
        FROM src_fields
    """)
    targ_fields3 = spark.sql("""
        SELECT * 
        FROM targ_fields
    """)

    columns = ['customerUniqueId', 'country', 'currencyCode']  # Example columns for scenario 3

    valid_columns = check_columns_and_log(src_fields3, targ_fields3, columns, "Scenario 3")

    src_to_tgt_diff = src_fields3.select(valid_columns).exceptAll(targ_fields3.select(valid_columns))
    tgt_to_src_diff = targ_fields3.select(valid_columns).exceptAll(src_fields3.select(valid_columns))

    return src_to_tgt_diff.unionByName(tgt_to_src_diff)

# Other scenarios follow the same pattern...

# Combine all scenarios
def combine_scenarios():
    # First, load customer and product data
    a_cus = read_customer_data()
    prod_qtx = read_product_data()

    # Create the prod_qtx view before running any scenarios
    create_prod_qtx(a_cus, prod_qtx)

    # Then, create the views for src_fields and targ_fields
    create_views()

    # Now, run each scenario
    comparison_1 = scenario_1()
    comparison_2 = scenario_2()
    comparison_3 = scenario_3()
    # Add other scenarios...

    # Combine all scenario comparisons
    combined_comparisons = comparison_1.unionByName(comparison_2)\
        .unionByName(comparison_3)
        # Add other comparisons...
    
    # Write the combined result to Parquet
    combined_comparisons.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# Run the combination of scenarios
combine_scenarios()

# Commit the Glue job
job.commit()