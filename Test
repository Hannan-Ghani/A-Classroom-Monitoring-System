import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.DoubleType
import scala.util.parsing.json.JSON

object DataValidationJob {

  // Function to check if a column exists in DataFrame
  def columnExists(df: DataFrame, colName: String): Boolean = df.columns.contains(colName)

  def main(sysArgs: Array[String]): Unit = {

    // Initialize SparkContext (AWS Glue automatically provides this)
    val sc: SparkContext = new SparkContext()
    val spark: SparkSession = SparkSession.builder().getOrCreate()

    // Load arguments from Glue job parameters
    val args = GlueArgParser.getResolvedOptions(
      sysArgs,
      Seq("SOURCE_FILE", "TARGET_FILE", "OUTPUT_FILE", "CONFIG_FILE").toArray
    )

    // Parse arguments
    val sourceFile = args("SOURCE_FILE")
    val targetFile = args("TARGET_FILE")
    val outputFile = args("OUTPUT_FILE")
    val configFile = args("CONFIG_FILE")  // Path to the JSON config file in S3

    // Load the JSON configuration file from S3
    val configDF = spark.read.json(configFile)

    // Convert the config DataFrame to a Map for easy access
    val configData = configDF.collect().map(row => {
      val rules = row.getSeq[Row](row.fieldIndex("rules")).map(rule => {
        Map(
          "source_column" -> rule.getAs[String]("source_column"),
          "target_column" -> rule.getAs[String]("target_column"),
          "transformation" -> rule.getAs[String]("transformation"),
          "type" -> rule.getAs[String]("type")
        )
      })
      Map("rules" -> rules)
    }).head

    // Load the source and target Parquet files directly from S3 as DataFrames
    val sourceDF = spark.read.parquet(sourceFile)
    val targetDF = spark.read.parquet(targetFile)

    // Apply transformations dynamically based on the config
    var transformedSourceDF = sourceDF

    for (rule <- configData("rules").asInstanceOf[List[Map[String, Any]]]) {
      val sourceCol = rule("source_column").toString
      val targetCol = rule("target_column").toString

      if (columnExists(transformedSourceDF, sourceCol)) {
        rule("transformation").toString match {
          case "copy" =>
            transformedSourceDF = transformedSourceDF.withColumn(targetCol, col(sourceCol))

          case "concat" =>
            val separator = rule.get("separator").map(_.toString).getOrElse("")
            val additionalCol = rule.get("additional_source_column").map(_.toString)
            if (additionalCol.isDefined && columnExists(transformedSourceDF, additionalCol.get)) {
              transformedSourceDF = transformedSourceDF.withColumn(targetCol, concat_ws(separator, col(sourceCol), col(additionalCol.get)))
            }

          case "conditional" =>
            val conditionField = rule("condition").asInstanceOf[Map[String, String]]("field")
            val conditionValue = rule("condition").asInstanceOf[Map[String, String]]("value")
            val elseValue = rule("condition").asInstanceOf[Map[String, String]]("else_value")
            if (columnExists(transformedSourceDF, conditionField)) {
              transformedSourceDF = transformedSourceDF.withColumn(targetCol,
                when(col(conditionField) === conditionValue, col(sourceCol)).otherwise(col(elseValue))
              )
            }

          case "cast" =>
            val dataType = rule.get("type").map(_.toString).getOrElse("string")
            if (dataType == "double") {
              transformedSourceDF = transformedSourceDF.withColumn(targetCol, col(sourceCol).cast(DoubleType))
            }
        }
      }
    }

    // Find common columns between source and target
    val commonColumns = targetDF.columns.toSet.intersect(transformedSourceDF.columns.toSet).toSeq

    // Get transformed column names from the config
    val transformedCommonColumns = configData("rules").asInstanceOf[List[Map[String, Any]]]
      .filter(rule => commonColumns.contains(rule("source_column").toString))
      .map(rule => rule("target_column").toString)

    // Join source and target on common columns (assuming transactionId is the key)
    if (commonColumns.contains("transactionId")) {
      val validationDF = transformedSourceDF.as("source").join(targetDF.as("target"), Seq("transactionId"), "inner")

      // Function to compare columns
      def compareColumns(col1: Column, col2: Column): Column = {
        when(col1.isNull && col2.isNull, true).otherwise(col1.cast("string") === col2.cast("string"))
      }

      // Create list of comparison columns
      val comparisonColumns = configData("rules").asInstanceOf[List[Map[String, Any]]]
        .filter(rule => commonColumns.contains(rule("source_column").toString))
        .map(rule => compareColumns(col(s"source.${rule("target_column")}"), col(s"target.${rule("target_column")}")).alias(s"${rule("target_column")}_match"))

      // Select comparison columns
      val comparisonDF = validationDF.select(col("transactionId") +: comparisonColumns: _*)

      // Filter mismatches
      val mismatchDF = comparisonDF.filter(!comparisonDF.columns.map(colName => col(colName)).reduce(_ && _))

      // Output mismatches or common columns if no mismatches
      if (mismatchDF.count() > 0) {
        mismatchDF.write.parquet(outputFile)
      } else {
        validationDF.select(transformedCommonColumns.head, transformedCommonColumns.tail: _*).write.parquet(outputFile)
      }
    } else {
      println("No common columns to join between source and target.")
    }
  }
}