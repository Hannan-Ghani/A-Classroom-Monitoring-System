To make the Glue Job even more dynamic by allowing it to read column names from a separate configuration file, you can introduce an additional .txt file for column configuration. This will provide flexibility to dynamically update the columns used in the transformations without modifying the Glue Job script.

Steps to Add Column Configuration
	1.	Create a Column Configuration File (columns_config.txt):
	•	This file will contain the column names and aliases for the transformations.
	2.	Modify the Glue Job Script:
	•	Parse the column configuration file to dynamically create SQL queries or select statements.

Updated Glue Job Script

Here’s the updated Glue Job script with dynamic column configuration:

import sys
import boto3
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_replace, lit, when, explode
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.utils import getResolvedOptions

# Parse arguments passed to the Glue Job
args = getResolvedOptions(sys.argv, ["JOB_NAME", "config_file", "columns_file"])
config_file = args["config_file"]  # Path to the main config file in S3
columns_file = args["columns_file"]  # Path to the columns config file in S3

# Initialize Glue and Spark contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = glueContext.create_job(args["JOB_NAME"])

# Helper function to read a file from S3
def read_s3_file(file_path):
    s3 = boto3.client("s3")
    bucket, key = file_path.replace("s3://", "").split("/", 1)
    return s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")

# Read and parse the main config file
config_data = read_s3_file(config_file)
config = {line.split("=")[0].strip(): line.split("=")[1].strip() for line in config_data.split("\n") if "=" in line}

# Read and parse the columns config file
columns_data = read_s3_file(columns_file)
columns_config = {line.split("=")[0].strip(): line.split("=")[1].strip() for line in columns_data.split("\n") if "=" in line}

# Extract file paths from the main config
customer_path = config["customer_path"]
product_path = config["product_path"]
target_path = config["target_path"]
output_path = config["output_path"]

# Extract column configurations
columns_src = columns_config["columns_src"].split(",")  # Columns for src_fields
columns_trgt = columns_config["columns_trgt"].split(",")  # Columns for targ_fields

# Read customer data
a_cus = (
    spark.read.parquet(customer_path)
    .filter(
        "NUM_OF_ACCOUNT >= 0 AND "
        "(SOURCE_COUNTRY <> 'ZA' OR SOURCE_COUNTRY IS NULL) AND "
        "customer_key IS NOT NULL AND "
        "customer_key NOT IN ('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')"
    )
    .withColumn(
        "customerUniqueId", 
        regexp_replace(col("customer_key"), "^.*-", "")  # Clean up 'customer_key'
    )
)

# Read product data
a_prod = (
    spark.read.parquet(product_path)
    .filter(
        (col("account_key").isNotNull() | col("customer_key").isNotNull()) &
        ~(
            (col("screening_system") == "YMUK2") &
            (
                (~col("customer_role").isin("CML - COMP PARENT", "PRIMARY")) |
                col("low_account_type").isin(
                    "UKCMLPHVI", "UKCMLPV100", "UKCMLPV5K", "UKCMLPV500", 
                    "UKCMLPV10K", "UKCMLPV1K", "UKCMLPVUNK", "UKCMLPBHI", 
                    "UKCMLPB100", "UKCMLPB5K", "UKCMLPB500", "UKCMLPB1K", "UKCMLPBUNK"
                )
            )
        )
    )
    .withColumn(
        "customerUniqueId", 
        regexp_replace(col("customer_key"), "^.*-", "")  # Clean up 'customer_key'
    )
)

# Join customer and product data
prod_qtx = (
    a_cus.join(a_prod, a_cus.customerUniqueId == a_prod.customerUniqueId, how="inner")
    .select(a_prod["*"])
)

# Read and process the target data
Qxta_targt = (
    spark.read.parquet(target_path)
    .select(
        "customerUniqueId", 
        explode(col("account")).alias("account")  # Explode the 'account' array
    )
    .select(
        "customerUniqueId", 
        "account.*"  # Expand all fields from the exploded 'account' struct
    )
)

# Select required fields for src_fields and targ_fields dynamically
src_fields = prod_qtx.select(*[col(field) for field in columns_src])
src_fields.createOrReplaceTempView("src_fields")
src_fields.cache()

selected_Qxta_targt = Qxta_targt.select(*[col(field) for field in columns_trgt])
selected_Qxta_targt.createOrReplaceTempView("selected_Qxta_targt")
selected_Qxta_targt.cache()

# Compare src_fields1 and targ_fields1
src_fields1 = src_fields.dropDuplicates()
targ_fields1 = selected_Qxta_targt.dropDuplicates()

# Find differences
src_diff = src_fields1.exceptAll(targ_fields1)
targ_diff = targ_fields1.exceptAll(src_fields1)

# Write differences to output path
src_diff.write.mode("overwrite").parquet(f"{output_path}/src_diff/")
targ_diff.write.mode("overwrite").parquet(f"{output_path}/targ_diff/")

# Commit Glue job
job.commit()

Example columns_config.txt

# Columns for src_fields
columns_src=customerUniqueId,account_risk_code_desc,accountName,accountIdPrefix

# Columns for targ_fields
columns_trgt=customerUniqueId,accountRiskCodeDescription as account_risk_code_desc,accountName,accountIdPrefix

How It Works
	1.	Configurable Columns:
	•	columns_src and columns_trgt specify the columns to be selected for src_fields and selected_Qxta_targt.
	•	The script dynamically builds .select() statements based on these configurations.
	2.	Dynamic Column Mapping:
	•	Aliases (e.g., accountRiskCodeDescription as account_risk_code_desc) can be defined directly in the column configuration file.
	3.	Simplified Maintenance:
	•	Updating column names requires only changing the columns_config.txt file.

How to Run
	1.	Upload both config.txt and columns_config.txt to your S3 bucket:
	•	Example paths:
	•	s3://your-config-bucket/config.txt
	•	s3://your-config-bucket/columns_config.txt
	2.	Pass their paths as Glue Job parameters:

--JOB_NAME MyGlueJob
--config_file s3://your-config-bucket/config.txt
--columns_file s3://your-config-bucket/columns_config.txt


	3.	Test the Glue Job to ensure it processes the data and selects columns dynamically.

Next Steps
	1.	Update your S3 buckets with example configuration files.
	2.	Run the Glue Job and validate the output files (src_diff and targ_diff).
	3.	Extend the configuration files as needed for additional use cases.