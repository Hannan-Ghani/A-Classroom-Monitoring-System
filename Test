import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.GlueArgParser
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.functions._
import org.apache.spark.sql.{Row, SaveMode, SparkSession}
import org.apache.spark.sql.types._
import scala.collection.JavaConverters._

object GlueApp {
  def main(sysArgs: Array[String]): Unit = {
    // Initialize GlueContext and SparkSession
    val sc: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(sc)
    val spark: SparkSession = glueContext.getSparkSession

    // Disable case sensitivity (optional, in case of case mismatches between columns)
    spark.conf.set("spark.sql.caseSensitive", "false")

    // Parse arguments
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("sourcePath", "targetPath", "outputPath", "configPath", "JOB_NAME").toArray)
    Job.init(args("JOB_NAME"), glueContext, args.asJava)

    // Paths
    val sourcePath = args("sourcePath")
    val targetPath = args("targetPath")
    val outputPath = args("outputPath")
    val configPath = args("configPath")

    // Log the paths
    println(s"Source Path: $sourcePath")
    println(s"Target Path: $targetPath")
    println(s"Config Path: $configPath")
    println(s"Output Path: $outputPath")

    try {
      // Load source and target parquet files
      val sourceDF = spark.read.parquet(s"$sourcePath")
      println("Source data loaded successfully")
      sourceDF.printSchema()  // Print schema for debugging

      val targetDF = spark.read.parquet(s"$targetPath")
      println("Target data loaded successfully")
      targetDF.printSchema()  // Print schema for debugging

      // Define schema for config JSON
      val configSchema = StructType(List(
        StructField("rules", ArrayType(StructType(List(
          StructField("source_column", StringType, true),
          StructField("target_column", StringType, true)
        ))), true),
        StructField("_corrupt_record", StringType, true)  // to handle corrupt records
      ))

      // Load the configuration file (rules) as JSON using defined schema
      val configDF = spark.read.schema(configSchema).json(s"$configPath")

      // Filter out corrupt records
      val corruptRecords = configDF.filter(col("_corrupt_record").isNotNull)
      if (corruptRecords.count() > 0) {
        println("Warning: Found corrupt records in the config file!")
        corruptRecords.show(false)
      }

      // Filter out valid records
      val validConfigDF = configDF.filter(col("_corrupt_record").isNull)

      // Process valid configuration data
      val configData = validConfigDF.collect().map(row => {
        val rules = row.getAs[Seq[Row]]("rules").map(rule => {
          Map(
            "source_column" -> rule.getAs[String]("source_column"),
            "target_column" -> rule.getAs[String]("target_column")
          )
        })
        Map("rules" -> rules)
      }).headOption.getOrElse(throw new Exception("Configuration data is missing or invalid."))

      // Create a sequence to store matching columns for output parquet
      var matchingColumns = Seq[String]()

      var differencesFound = false

      for (rule <- configData("rules").asInstanceOf[List[Map[String, Any]]]) {
        val sourceCol = rule("source_column").toString
        val targetCol = rule("target_column").toString

        // Check if both the source and target DataFrames have the specified columns
        if (sourceDF.columns.contains(sourceCol) && targetDF.columns.contains(targetCol)) {
          // Add matching columns to the list
          matchingColumns = matchingColumns :+ targetCol

          // Compare the source and target columns
          val sourceColData = sourceDF.select(sourceCol).as("source")
          val targetColData = targetDF.select(targetCol).as("target")

          // Find any differences between source and target for this column
          val diff = sourceColData.except(targetColData)
          
          if (diff.count() > 0) {
            println(s"Differences found in column: $sourceCol -> $targetCol")
            diff.show(false)
            differencesFound = true
          }
        } else {
          println(s"Either $sourceCol in source or $targetCol in target is missing, skipping comparison.")
        }
      }

      // Generate the output DataFrame for columns that were successfully matched
      val emptyRows = spark.createDataFrame(
        spark.sparkContext.parallelize(Seq.empty[Row]), 
        targetDF.schema
      )

      // If no differences are found, create an empty DataFrame with the matching column names
      if (!differencesFound) {
        val emptyDF = emptyRows.select(matchingColumns.map(col): _*)
        println("No differences found between source and target data.")
        emptyDF.write.format("parquet").mode(SaveMode.Overwrite).save(s"$outputPath/no_differences_found")
      } else {
        println("Differences found, no empty confirmation file created.")
      }

    } catch {
      case e: NullPointerException =>
        println("NullPointerException occurred: " + e.getMessage)
        throw e  // rethrow to ensure Glue captures it
      case e: Exception =>
        println("An error occurred: " + e.getMessage)
        throw e
    }

    // Commit the job
    Job.commit()
  }
}