from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, udf, lit, when, trim
from pyspark.sql.functions import min as spark_min, max as spark_max
import scala.io.Source

# Initialize SparkSession
spark = SparkSession.builder.appName("CleanseCaseClass").getOrCreate()

# ==============================
# Configuration Parameters
# ==============================

# Define paths for local testing or use absolute paths to S3 if AWS credentials are configured locally
s3ConfigPath = "path/to/local/paths_config.txt"
s3ColumnsConfigPath = "path/to/local/columns_config.txt"
outputSummaryPath = "path/to/local/output_summary.txt"

# ==============================
# Load Configurations
# ==============================
# Load path configuration from a local text file
configMap = {}
with open(s3ConfigPath, 'r') as f:
    for line in f:
        key, value = line.strip().split('=')
        configMap[key.strip()] = value.strip()

inputSourcePath = configMap["inputSourcePath"]
inputTargetPath = configMap["inputTargetPath"]
outputBasePath = configMap["outputBasePath"]

# Load column configurations from a local text file
columnConfig = {}
with open(s3ColumnsConfigPath, 'r') as f:
    for line in f:
        key, value = line.strip().split('=')
        columnConfig[key.strip()] = value.strip().split(',')

# ==============================
# Load Data from Paths
# ==============================
# Use local paths or S3 paths if accessible
transactionSource = spark.read.parquet(inputSourcePath)
caseClass = spark.read.parquet(inputTargetPath)

# Initialize validation summary string
validationSummary = "Validation Results Summary:\n"

# ==============================
# Country Code Mapping UDF
# ==============================
countryCodeMapping = {
    "US": "USA", "CA": "CAN", "MX": "MEX", "GB": "GBR", "FR": "FRA",
    "DE": "DEU", "IN": "IND", "CN": "CHN", "JP": "JPN", "AU": "AUS"
    # Add more mappings as needed
}

def map_country_code(code):
    return countryCodeMapping.get(code, code)

# Register the UDF in Spark
mapCountryCodeUDF = udf(map_country_code)

# ==============================
# Null Value Validation
# ==============================
nullColumns = columnConfig.get("null_validation.columns", [])
existingNullColumns = [col for col in nullColumns if col in caseClass.columns]

nullValidationCount = caseClass.filter(lambda row: any(row[col] is None for col in existingNullColumns)).count()
validationSummary += f"Null Validation: {nullValidationCount} found\n"

# ==============================
# Direct Column Validation
# ==============================
sourceDirectColumns = columnConfig.get("direct_column_validation.columns_source", [])
targetDirectColumns = columnConfig.get("direct_column_validation.columns_target", [])

directSourceData = transactionSource.select([col(c) for c in sourceDirectColumns])
directTargetData = caseClass.select([col(c) for c in targetDirectColumns])

# Source-to-target and target-to-source differences
directSourceToTargetDiff = directSourceData.exceptAll(directTargetData)
directTargetToSourceDiff = directTargetData.exceptAll(directSourceData)

# Collect counts and IDs
directSourceToTargetCount = directSourceToTargetDiff.count()
directTargetToSourceCount = directTargetToSourceDiff.count()
directSourceToTargetIds = ", ".join(directSourceToTargetDiff.select("TRANSACTION_ID").rdd.flatMap(lambda x: x).collect())
directTargetToSourceIds = ", ".join(directTargetToSourceDiff.select("transactionId").rdd.flatMap(lambda x: x).collect())

validationSummary += f"Direct Column Validation: {directSourceToTargetCount + directTargetToSourceCount} found\n"
validationSummary += f"Source to Target Transaction IDs: {directSourceToTargetIds}\n"
validationSummary += f"Target to Source Transaction IDs: {directTargetToSourceIds}\n"

# ==============================
# Country ISO3 Validation with Mapping
# ==============================
sourceCountryColumn = columnConfig["transaction_country_iso3_validation.source_column"][0]
targetCountryColumn = columnConfig["transaction_country_iso3_validation.target_column"][0]

# Apply country code mapping to source data
sourceCountryData = transactionSource.withColumn("isoCountryCodeMapped", mapCountryCodeUDF(col(sourceCountryColumn)))
targetCountryData = caseClass.select(col("transactionId"), col(targetCountryColumn).alias("isoCountryCode"))

countrySourceToTargetDiff = sourceCountryData.select("TRANSACTION_ID", "isoCountryCodeMapped").exceptAll(targetCountryData)
countryTargetToSourceDiff = targetCountryData.exceptAll(sourceCountryData.select("TRANSACTION_ID", "isoCountryCodeMapped"))

# Count and collect IDs for differences
countrySourceToTargetCount = countrySourceToTargetDiff.count()
countryTargetToSourceCount = countryTargetToSourceDiff.count()
countrySourceToTargetIds = ", ".join(countrySourceToTargetDiff.select("TRANSACTION_ID").rdd.flatMap(lambda x: x).collect())
countryTargetToSourceIds = ", ".join(countryTargetToSourceDiff.select("transactionId").rdd.flatMap(lambda x: x).collect())

validationSummary += f"Country ISO3 Validation: {countrySourceToTargetCount + countryTargetToSourceCount} found\n"
validationSummary += f"Source to Target Transaction IDs: {countrySourceToTargetIds}\n"
validationSummary += f"Target to Source Transaction IDs: {countryTargetToSourceIds}\n"

# ==============================
# Write Validation Summary to Local Output
# ==============================
with open(outputSummaryPath, 'w') as f:
    f.write(validationSummary)

# Stop Spark session
spark.stop()
