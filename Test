import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

# Initialize Glue context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Define the base S3 output path
output_path = args['OUTPUT_PATH']

# Sample data for Customer
customer_data = [
    ('CUST001', 3, 'US', 'ACC001', '123456', 'USD', 'BANK01', 'true', '2023-10-01', 'LOB1', 'LOB Desc 1', '2025-01-01', 'false', 'false', '12345678', 'source1', 'EXTERNAL', 'INDUSTRY_CODE1', 'LEGAL_ENTITY1'),
    ('CUST002', 2, 'UK', 'ACC002', '654321', 'GBP', 'BANK02', 'false', '2023-09-01', 'LOB2', 'LOB Desc 2', '2026-02-01', 'true', 'true', '87654321', 'source2', 'INTERNAL', 'INDUSTRY_CODE2', 'LEGAL_ENTITY2'),
    ('CUST003', 5, 'ZA', 'ACC003', '987654', 'ZAR', 'BANK03', 'true', '2023-08-01', 'LOB3', 'LOB Desc 3', '2024-03-01', 'false', 'true', '13579246', 'source3', 'EXTERNAL', 'INDUSTRY_CODE3', 'LEGAL_ENTITY3')
]

# Sample data for Product Fortent
product_fortent_data = [
    ('ACC001', 'TMUK2', 'PRIMARY', 'CML - COMP PARENT', 'UKCMPVHI', '12345678', 'high_type1', 'risk_desc1'),
    ('ACC002', 'TMUK2', 'CML - COMP PARENT', 'PRIMARY', 'UKCMLPV5K', '87654321', 'high_type2', 'risk_desc2'),
    ('ACC004', 'OTHER', 'PRIMARY', 'OTHER', 'UKCMLPVUNK', '54321098', 'low_type1', 'risk_desc3')
]

# Sample data for Product Core
product_core_data = [
    ('ACC001', 'CUST001', 'HIGH', 'BANK01', 'USD', 'EXTERNAL', 'INDUSTRY_CODE1'),
    ('ACC003', 'CUST003', 'LOW', 'BANK03', 'ZAR', 'INTERNAL', 'INDUSTRY_CODE3'),
    ('ACC005', 'CUST005', 'MEDIUM', 'BANK05', 'EUR', 'EXTERNAL', 'INDUSTRY_CODE5')
]

# Sample data for Document
document_data = [
    ('CUST001', 'ACC001', 'DOC001'),
    ('CUST002', 'ACC002', 'DOC002'),
    ('CUST003', 'ACC003', 'DOC003')
]

# Convert each dataset to DataFrames, including all columns from your original code

# Customer DataFrame
customer_df = spark.createDataFrame(customer_data, [
    'customer_key', 'NUM_OF_ACCOUNT', 'SOURCE_COUNTRY', 'account_key', 
    'account_no', 'currency_code', 'financial_institution', 'joint_account', 
    'lastUpdatedDate', 'line_of_business', 'line_of_business_desc', 'maturity_date', 
    'non_operating_entity', 'numbered_account', 'sortcode', 'source_sys_code', 
    'external_account_type', 'industry_code_desc', 'legal_entity'
])

# Product Fortent DataFrame
product_fortent_df = spark.createDataFrame(product_fortent_data, [
    'account_key', 'screening_system', 'customer_role', 'low_account_type', 
    'high_account_type', 'sortcode', 'high_account_type_desc', 'account_risk_code_desc'
])

# Product Core DataFrame
product_core_df = spark.createDataFrame(product_core_data, [
    'account_key', 'customer_key', 'account_risk_code_desc', 
    'financial_institution', 'currency_code', 'external_account_type', 
    'industry_code'
])

# Document DataFrame
document_df = spark.createDataFrame(document_data, [
    'customerUniqueId', 'account_key', 'document_id'
])

# Write each DataFrame to its respective Parquet file, categorized by type
customer_df.write.mode('overwrite').parquet(f"{output_path}/customer/")
product_fortent_df.write.mode('overwrite').parquet(f"{output_path}/product_fortent/")
product_core_df.write.mode('overwrite').parquet(f"{output_path}/product_core/")
document_df.write.mode('overwrite').parquet(f"{output_path}/document/")

# Commit the Glue job
job.commit()