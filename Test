The “cannot resolve column name” error you’re getting in your main code might occur because the columns expected in your Glue job are not present in the Parquet files you generated. Here are a few steps to troubleshoot and ensure you’re resolving the issue:

1. Check Column Names in Generated Parquet Files

Ensure that the column names in your generated Parquet files exactly match the column names expected by the main code. Parquet files are case-sensitive, so even small variations in capitalization (e.g., account_key vs accountKey) can lead to errors.

You can check the columns of the generated files using the following code:

df = spark.read.parquet("s3://path-to-your-parquet-file/")
df.printSchema()

2. Compare Columns in Your Main Code and Parquet Files

Ensure that the columns listed in your main code match the ones in the generated Parquet files. If any are missing, add them either to your Parquet generation script or modify the main code to handle the missing columns gracefully (by skipping or logging them).

3. Log Missing Columns in the Glue Job

Modify your main code to log any missing columns. You can handle this by comparing the expected columns with the actual columns read from the input data.

Here’s an example of how to dynamically handle missing columns in your main code:

expected_columns = ['customer_key', 'account_key', 'NUM_OF_ACCOUNT', ...]  # List all columns here
actual_columns = df.columns

missing_columns = set(expected_columns) - set(actual_columns)

if missing_columns:
    print(f"Missing columns: {missing_columns}")
else:
    print("All expected columns are present.")

4. Ensure All Data Types Match

Another possible cause of the error could be mismatched data types. Ensure that the data types in the generated Parquet files are consistent with what your main code expects.

5. Modify the Main Code to Be More Resilient

If some columns are optional or may not always be present, you could modify the main code to skip over missing columns rather than failing with an exception.

Here is an example of how to handle missing columns dynamically:

# Check for missing columns before querying
expected_columns = ['account_key', 'customer_key', 'currency_code', 'financial_institution']

df = spark.read.parquet('s3://your-bucket/customer/')

# Ensure only columns that exist are selected
available_columns = [col for col in expected_columns if col in df.columns]

if len(available_columns) < len(expected_columns):
    missing_columns = set(expected_columns) - set(df.columns)
    print(f"Missing columns: {missing_columns}")

# Select only the available columns
df = df.select(available_columns)

Next Steps:

	1.	Check the columns of the generated Parquet files to ensure they match your main code.
	2.	Update your main Glue job to either add missing columns or handle them gracefully by logging and skipping those columns.
	3.	Check the case-sensitivity of your column names.

After making these changes, test your Glue job again. Let me know if you need any specific guidance on modifying your main code.