def validateNonNullColumns(df: DataFrame, columns: Seq[String]): Seq[String] = {
  import df.sparkSession.implicits._
  val existingColumns = filterExistingColumns(df, columns)

  if (existingColumns.nonEmpty) {
    // Store results as Seq of formatted strings
    val nonNullValues = existingColumns.flatMap { column =>
      val nonNullData = df.select($"transactionId", col(column))  // Select transactionId and the target column
        .filter(col(column).isNotNull)  // Filter where target column is not null

      // Collect transaction IDs and corresponding non-null values
      val nonNullRecords = nonNullData.collect().map(row => (row.getString(0), row.getString(1)))  // (transactionId, column value)
      
      if (nonNullRecords.nonEmpty) {
        val transactionIds = nonNullRecords.map(_._1).mkString(", ")
        Some(s"Non-Null Validation for $column: ${nonNullRecords.length} found with transaction IDs: " + transactionIds)
      } else {
        None
      }
    }
    Seq("Non-Null Validation Differences:") ++ nonNullValues.flatten
  } else {
    Seq("Non-Null Validation: No relevant columns found for non-null validation.")
  }
}

[non_null_validation]
aAccountNumberCleansed, anotherColumn, moreColumns
