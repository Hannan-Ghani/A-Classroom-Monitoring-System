import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, min, max
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job

# Initialize Glue job
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
spark = SparkSession.builder.appName('GlueJob').getOrCreate()
glueContext = GlueContext(spark.sparkContext)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Reading and filtering transaction data
transaction_source = spark.read.parquet("s3://136731789529-fis-raw-data/transaction/raw/parquet/") \
    .filter((col("transaction_date") >= "2023-09-15") & (col("transaction_date") <= "2023-09-28"))

# Saving the count result to S3
transaction_source \
    .agg(count("*").alias("transaction_count")) \
    .write.mode("overwrite").csv("s3://your-output-bucket/path/transaction_count.csv")

# Reading Case Class data
transaction_case = spark.read.parquet("s3://136731789529-fis-interim-data/transaction/2023-10-25_10-46-40-587/CaseClass/fullRawCaseClass.parquet/")
transaction_case \
    .agg(count("*").alias("case_count")) \
    .write.mode("overwrite").csv("s3://your-output-bucket/path/case_count.csv")

# Continue for other datasets as needed...

# Finish job
job.commit()
spark.stop()