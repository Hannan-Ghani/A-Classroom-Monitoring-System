import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.sql.functions._

// Initialize Glue and Spark context
val spark: SparkContext = new SparkContext()
val glueContext: GlueContext = new GlueContext(spark)
val sparkSession: SparkSession = glueContext.getSparkSession
import sparkSession.implicits._

// Get the S3 config path from Glue job parameters
val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3ConfigPath").toArray)
val s3ConfigPath = args("s3ConfigPath")

// Read the configuration file from S3
val configDF = sparkSession.read.text(s3ConfigPath)

// Convert the config DataFrame into a Map of key-value pairs
val configMap = configDF.collect().map { row =>
  val split = row.getString(0).split("=")
  (split(0), split(1))
}.toMap

// Extract input and output paths from the config file
val inputSourcePath = configMap("inputSourcePath")
val inputTargetPath = configMap("inputTargetPath")
val outputSourceExceptPath = configMap("outputSourceExceptPath")
val outputTargetExceptPath = configMap("outputTargetExceptPath")

// Step 1: Read the transaction source Parquet file from the inputSourcePath in the config file
val transactionSource: DataFrame = sparkSession.read.parquet(inputSourcePath)
  .filter($"transaction_date" > "2023-01-01" && $"transaction_date" < "2023-01-31")

// Count the number of rows in the filtered DataFrame
val transactionCount = transactionSource.count()
println(s"Number of transactions: $transactionCount")

// Inspect the schema of the transactionSource DataFrame
transactionSource.printSchema()

// Find the minimum transaction date
transactionSource.select(min($"transaction_date")).show(false)

// Find the maximum transaction date
transactionSource.select(max($"transaction_date")).show(false)

// Step 2: Read the case class DataFrame from the inputTargetPath in the config file
val caseClass: DataFrame = sparkSession.read.parquet(inputTargetPath)

// Select relevant columns from the transactionSource DataFrame
val source = transactionSource.select(
  $"TRANSACTION_ID",
  $"TRANSACTION_TYPE",
  $"TRANSACTION_TYPE_DESC",
  $"SHORTNARRATIVE", 
  $"TRANSACTION_CODE",
  $"TRANSACTION_CODE_DESC",
  $"SOURCE_TRANSACTION_ID", 
  $"ORIGINAL_CURRENCY_CODE",
  $"TRANSACTION_COUNTRY",
  $"TRANSACTION_LOCATION", 
  $"screening_system"
)

// Select relevant columns from the caseClass DataFrame
val target = caseClass.select(
  $"transactionId",
  $"transactionType", 
  $"transactionTypeDescription", 
  $"narrativeShort",
  $"transactionCode",
  $"transactionCodeDescription", 
  $"sourceTransactionId",
  $"currencyLocal",
  $"transactionCountry",
  $"transactionLocation",
  $"sourceSystem"
)

// Step 3: Compare the source and target DataFrames using 'except' to find differences
val source_except = source.except(target)
val target_except = target.except(source)

// Display the rows that are in source but not in target
source_except.show(2, false)

// Display the rows that are in target but not in source
target_except.show(2, false)

// Step 4: Write the result to S3 as Parquet files (using paths from the config file)
source_except.write.mode("overwrite").parquet(outputSourceExceptPath)
target_except.write.mode("overwrite").parquet(outputTargetExceptPath)

// Commit the Glue job
Job.commit()