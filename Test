import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.sql.functions._
import org.apache.log4j.Logger

object CleanseCaseClass {

  def main(sysArgs: Array[String]): Unit = {
    // Initialize Glue and Spark context
    val spark: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(spark)
    val sparkSession: SparkSession = glueContext.getSparkSession
    import sparkSession.implicits._

    // Logger setup
    val logger: Logger = Logger.getLogger("CleanseCaseClassLogger")

    // Get the S3 config path from Glue job parameters
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("s3ConfigPath").toArray)
    val s3ConfigPath = args("s3ConfigPath")

    // Read the configuration file from S3
    val configDF = sparkSession.read.text(s3ConfigPath)

    // Convert the config DataFrame into a Map of key-value pairs
    val configMap = configDF.collect().map { row =>
      val split = row.getString(0).split("=")
      (split(0), split(1))
    }.toMap

    // Extract input and output paths from the config file
    val inputSourcePath = configMap("inputSourcePath")
    val inputTargetPath = configMap("inputTargetPath")
    val outputCombinedExceptPath = configMap("outputCombinedExceptPath")

    // =======================
    // PART 1: Read Source and Target Data
    // =======================

    // Read the transaction source Parquet file from the inputSourcePath in the config file
    val transactionSource: DataFrame = sparkSession.read.parquet(inputSourcePath)
      .filter($"transaction_date" > "2023-01-01" && $"transaction_date" < "2023-01-31")

    // Read the case class DataFrame from the inputTargetPath in the config file
    val caseClass: DataFrame = sparkSession.read.parquet(inputTargetPath)

    // =======================
    // PART 2: Handling Missing Columns and Logging
    // =======================

    // Function to filter valid columns and log missing ones without breaking mapping
    def filterValidColumns(sourceCols: Seq[String], targetCols: Seq[String], sourceDF: DataFrame, targetDF: DataFrame, description: String): Seq[(String, String)] = {
      sourceCols.zip(targetCols).filter { case (sourceCol, targetCol) =>
        val sourceHasCol = sourceDF.columns.contains(sourceCol)
        val targetHasCol = targetDF.columns.contains(targetCol)

        if (!sourceHasCol) {
          logger.warn(s"Column '$sourceCol' is missing in source for $description. Skipping this column pair.")
        }
        if (!targetHasCol) {
          logger.warn(s"Column '$targetCol' is missing in target for $description. Skipping this column pair.")
        }
        sourceHasCol && targetHasCol
      }
    }

    // =======================
    // PART 3: Narrative Validation
    // =======================

    // Clean the "NARRATIVE" column in the source by replacing empty strings or asterisks with null
    val source_Narrative = transactionSource.withColumn(
      "NARRATIVE", 
      when(trim($"NARRATIVE") === "", null)
        .when(trim($"NARRATIVE") === "*", null)
        .otherwise($"NARRATIVE")
    ).select($"TRANSACTION_ID", $"NARRATIVE")

    // Select the corresponding narrative columns from the target
    val target_Narrative = caseClass.select($"transactionId", $"narrative")

    // Perform source-to-target validation using exceptAll
    val source_to_target_narrative_diff = source_Narrative.exceptAll(target_Narrative)

    // Perform target-to-source validation using exceptAll
    val target_to_source_narrative_diff = target_Narrative.exceptAll(source_Narrative)

    // =======================
    // PART 4: Null Value Validation (Target Only)
    // =======================
    
    // Select the 41 columns from target for null validation
    val targetNullColumns = Seq( /* Placeholder for 41 target columns */ )

    // Select and use distinct() for target null validation columns
    val validTargetNullColumns = targetNullColumns.filter(colName => {
      if (!caseClass.columns.contains(colName)) {
        logger.warn(s"Column '$colName' is missing in target data for null validation.")
        false
      } else {
        true
      }
    })

    val targetNullData = caseClass.select(validTargetNullColumns.map(col): _*).distinct()

    // =======================
    // PART 5: Direct Column Validation (Constant Mapping)
    // =======================

    // Define source and target columns for direct validation
    val sourceDirectColumns = Seq(
      "TRANSACTION_ID", "TRANSACTION_TYPE", "TRANSACTION_TYPE_DESC", "SHORTNARRATIVE",
      "TRANSACTION_CODE", "TRANSACTION_CODE_DESC", "SOURCE_TRANSACTION_ID",
      "ORIGINAL_CURRENCY_CODE", "TRANSACTION_COUNTRY", "TRANSACTION_LOCATION", "screening_system"
    )

    val targetDirectColumns = Seq(
      "transactionId", "transactionType", "transactionTypeDescription", "narrativeShort",
      "transactionCode", "transactionCodeDescription", "sourceTransactionId",
      "currencyLocal", "transactionCountry", "transactionLocation", "sourceSystem"
    )

    // Filter out missing columns without breaking the original mapping
    val validDirectColumns = filterValidColumns(sourceDirectColumns, targetDirectColumns, transactionSource, caseClass, "direct validation")

    // Perform source-to-target validation using exceptAll
    val sourceDirectData = transactionSource.select(validDirectColumns.map { case (sourceCol, _) => col(sourceCol) }: _*)
    val targetDirectData = caseClass.select(validDirectColumns.map { case (_, targetCol) => col(targetCol) }: _*)
    val direct_source_to_target_diff = sourceDirectData.exceptAll(targetDirectData)

    // Perform target-to-source validation using exceptAll
    val direct_target_to_source_diff = targetDirectData.exceptAll(sourceDirectData)

    // =======================
    // PART 6: Combine Results and Write to S3
    // =======================

    // Combine all results
    val combined_results = targetNullData
      .union(source_to_target_narrative_diff)
      .union(target_to_source_narrative_diff)
      .union(direct_source_to_target_diff)
      .union(direct_target_to_source_diff)

    // Write the combined results to S3 as a Parquet file
    combined_results.write.mode("overwrite").parquet(outputCombinedExceptPath)

    // Commit the Glue job to mark it as successfully completed
    Job.commit()
  }
}