import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

object GlueApp {
  def main(sysArgs: Array[String]): Unit = {
    // Initialize Spark and Glue contexts
    val sc: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(sc)
    val spark: SparkSession = glueContext.getSparkSession

    // Parse job parameters
    val args = GlueArgParser.getResolvedOptions(sysArgs, Array(
      "JOB_NAME",
      "SOURCE_PATH_CUSTOMER",
      "SOURCE_PATH_PRODUCT_CORE",
      "SOURCE_PATH_PRODUCT_FORTENT",
      "OUTPUT_PATH"
    ))

    val jobName = args("JOB_NAME")
    val customerPath = args("SOURCE_PATH_CUSTOMER")
    val productCorePath = args("SOURCE_PATH_PRODUCT_CORE")
    val productFortentPath = args("SOURCE_PATH_PRODUCT_FORTENT")
    val outputPath = args("OUTPUT_PATH")

    // Read customer data
    val aCus = spark.read.parquet(customerPath)
      .filter("NUM_OF_ACCOUNT >= 0 AND (SOURCE_COUNTRY <> 'ZA' OR SOURCE_COUNTRY IS NULL) AND customer_key IS NOT NULL")
      .withColumn("customerUniqueId", regexp_replace(col("customer_key"), "[^\\w]", ""))
    
    aCus.createOrReplaceTempView("a_cus")

    // Read product data from both sources
    val prodCore = spark.read.parquet(productCorePath)
    prodCore.createOrReplaceTempView("Prod_core")

    val prodFortent = spark.read.parquet(productFortentPath)
    prodFortent.createOrReplaceTempView("Prod_fortent")

    // Union of product data
    val prodJoin = spark.sql("SELECT * FROM Prod_core UNION SELECT * FROM Prod_fortent")
    prodJoin.createOrReplaceTempView("prodJoin")

    // Filter joined product data
    val aProd = prodJoin.filter(
      (col("account_key").isNotNull || col("customer_key").isNotNull) &&
      (!col("customer_role").isin("CML - COMP PARENT", "PRIMARY")) &&
      col("low_account_type").isin(
        "UKCMLPVHI", "UKCMLPV100", "UKCMLPV5K", "UKCMLPV500", "UKCMLPV10K",
        "UKCMLPV1K", "UKCMLPVUNK", "UKCMLPBHI", "UKCMLPB100", "UKCMLPB5K",
        "UKCMLPB500", "UKCMLPB1K", "UKCM"
      )
    ).withColumn("customerUniqueId", regexp_replace(col("customer_key"), "[^\\w]", ""))

    aProd.createOrReplaceTempView("a_prod")

    // Join customer and product data
    val prodQtx = spark.sql("""
      SELECT * 
      FROM a_cus 
      INNER JOIN a_prod ON a_cus.customerUniqueId = a_prod.customerUniqueId
    """).dropDuplicates()

    prodQtx.createOrReplaceTempView("prod_qtx")

    // Write results to the specified output path
    prodQtx.write.mode("overwrite").parquet(outputPath)

    println("ETL job completed successfully.")
  }
}