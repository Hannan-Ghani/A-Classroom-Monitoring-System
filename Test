import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.Job
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.sql.{DataFrame, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.SparkContext
import org.apache.spark.sql.types.{StructField, StructType, StringType}
import scala.collection.JavaConverters._

object RuleBasedProcessing {
  def main(sysArgs: Array[String]): Unit = {
    // Initialize GlueContext and SparkContext
    val sc: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(sc)
    val spark = glueContext.getSparkSession

    // Parse arguments (configPath and outputPath from job parameters)
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("configPath", "outputPath", "sourcePath", "targetPath", "JOB_NAME").toArray)
    Job.init(args("JOB_NAME"), glueContext, args.asJava)

    // Load the paths from job parameters
    val configPath = args("configPath")
    val sourcePath = args("sourcePath")
    val targetPath = args("targetPath")
    val outputPath = args("outputPath")

    // Read the rules from the text file
    val rules = spark.read.textFile(configPath).collect()

    // Parse the rules into a Map for easy access
    val ruleMap = rules.map { line =>
      val Array(sourceColumn, targetColumn) = line.split(":")
      sourceColumn -> targetColumn
    }.toMap

    println("Parsed Rules:")
    ruleMap.foreach(println)

    // Load source and target DataFrames
    val sourceDF = spark.read.parquet(sourcePath)
    val targetDF = spark.read.parquet(targetPath)

    // Create an empty DataFrame for the result
    var resultDF: DataFrame = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], StructType(Seq.empty[StructField]))

    // Iterate over the rules and compare the columns that are present in both source and target
    ruleMap.foreach { case (sourceColumn, targetColumn) =>
      if (sourceDF.columns.contains(sourceColumn) && targetDF.columns.contains(targetColumn)) {
        println(s"Comparing $sourceColumn in source with $targetColumn in target...")

        // Select the relevant columns from source and target
        val sourceColData = sourceDF.select(sourceColumn).withColumnRenamed(sourceColumn, "source_value")
        val targetColData = targetDF.select(targetColumn).withColumnRenamed(targetColumn, "target_value")

        // Combine source and target data for comparison
        val combinedDF = sourceColData.join(targetColData, sourceColData("source_value") === targetColData("target_value"), "full_outer")
          .withColumn("difference", when(col("source_value") =!= col("target_value"), lit("DIFFERENT")).otherwise(lit("MATCH")))

        // Filter only the rows where differences exist
        val diffDF = combinedDF.filter("difference = 'DIFFERENT'")
          .select(col("source_value").alias(sourceColumn), col("target_value").alias(targetColumn))

        // Union with the result DataFrame (accumulate differences)
        resultDF = resultDF.union(diffDF)
      } else {
        println(s"Skipping comparison for $sourceColumn (source) and $targetColumn (target) because one or both columns are missing.")
      }
    }

    // If there are no differences, produce an empty DataFrame with the matching columns
    if (resultDF.isEmpty) {
      println("No differences found between source and target data. Producing empty output with matching columns.")

      // Get the matching columns for both source and target
      val matchingColumns = ruleMap.filter { case (sourceCol, targetCol) =>
        sourceDF.columns.contains(sourceCol) && targetDF.columns.contains(targetCol)
      }.map(_._2).toSeq

      // Create an empty DataFrame with these matching columns
      val emptySchema = StructType(matchingColumns.map(c => StructField(c, StringType, nullable = true)))
      val emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], emptySchema)

      // Write the empty DataFrame with matching columns to the output path
      emptyDF.write.mode("overwrite").parquet(outputPath + "/no_differences_found")
    } else {
      // Write the result DataFrame with differences to the output path
      resultDF.write.mode("overwrite").parquet(outputPath + "/differences_found")
    }

    // Commit the job
    Job.commit()
  }
}