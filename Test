import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import scala.util.Try

def applyDirectColumnValidation(
    sourceCols: String,
    targetCols: String,
    transactionSource: DataFrame,
    caseClass: DataFrame
)(implicit spark: SparkSession): List[String] = {
  import spark.implicits._

  // Split and align column names
  val sourceColumns = sourceCols.split(",").map(_.trim).toSeq
  val targetColumns = targetCols.split(",").map(_.trim).toSeq

  // Ensure equal column counts
  if (sourceColumns.length != targetColumns.length) {
    throw new IllegalArgumentException("Source and target column lists must have the same length.")
  }

  // Initialize lists to track missing columns
  val missingSourceCols = sourceColumns.filterNot(transactionSource.columns.contains)
  val missingTargetCols = targetColumns.filterNot(caseClass.columns.contains)

  // Log missing columns if any
  if (missingSourceCols.nonEmpty || missingTargetCols.nonEmpty) {
    println(s"Missing source columns: ${missingSourceCols.mkString(", ")}")
    println(s"Missing target columns: ${missingTargetCols.mkString(", ")}")
  }

  // Filter only available columns to continue processing
  val filteredSourceCols = sourceColumns.diff(missingSourceCols)
  val filteredTargetCols = targetColumns.diff(missingTargetCols)

  // Select and align columns, cast to string for uniform comparison
  val sourceSelected = transactionSource.select(filteredSourceCols.zipWithIndex.map {
    case (colName, i) => col(colName).cast("string").as(s"col_$i")
  }: _*)

  val targetSelected = caseClass.select(filteredTargetCols.zipWithIndex.map {
    case (colName, i) => col(colName).cast("string").as(s"col_$i")
  }: _*)

  // Intersect to find matching rows
  val matchingData = sourceSelected.intersect(targetSelected)

  // Collect transaction IDs from matching rows (first column is col_0)
  matchingData.select("col_0").as[String].collect().toList
}