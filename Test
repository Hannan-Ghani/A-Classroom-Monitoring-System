import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.GlueArgParser
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

object GlueApp {
  def main(sysArgs: Array[String]): Unit = {
    // Initialize Spark and Glue contexts
    val sc: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(sc)
    val spark: SparkSession = glueContext.getSparkSession

    // Parse job parameters from AWS Glue
    val args = GlueArgParser.getResolvedOptions(sysArgs, Array(
      "SOURCE_PATH",  // Source Parquet file path
      "TARGET_PATH",  // Target Parquet file path
      "OUTPUT_PATH"   // Output path for the comparison result
    ))

    // Extract paths from parameters
    val sourceFilePath = args("SOURCE_PATH")
    val targetFilePath = args("TARGET_PATH")
    val outputPath = args("OUTPUT_PATH")

    // Load the Parquet files into DataFrames
    val source = spark.read.parquet(sourceFilePath)
    val target = spark.read.parquet(targetFilePath)

    // Get the list of common columns between source and target
    val commonColumns = source.columns.toSet.intersect(target.columns.toSet).toSeq

    // Select the common columns from both source and target DataFrames
    val sourceCommon = source.select(commonColumns.map(col): _*)
    val targetCommon = target.select(commonColumns.map(col): _*)

    // Perform the comparison for each common column
    val comparison = commonColumns.foldLeft(sourceCommon.join(targetCommon, Seq("transactionId"), "outer")) { (df, colName) =>
      df.withColumn(
        s"${colName}_comparison",
        when(sourceCommon(colName) === targetCommon(colName), "MATCH").otherwise("DIFFERENT")
      )
    }

    // Write the comparison result to the specified output path
    comparison.write.mode("overwrite").parquet(outputPath)

    println("ETL job completed successfully.")
  }
}