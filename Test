import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import com.amazonaws.services.glue.util.GlueArgParser

object TransactionProcessing {

  def main(sysArgs: Array[String]): Unit = {
    // Initialize Spark Session
    val spark = SparkSession.builder.appName("TransactionProcessing").getOrCreate()

    // Retrieve job parameters from Glue job configuration
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq(
      "SOURCE_PATH",           // S3 path for source data
      "TARGET_PATH",           // S3 path for target output
      "BATCH_ID",              // Batch ID for filtering
      "OUTPUT_BUCKET"          // S3 output bucket for saving results
    ).toArray)

    // Extract parameter values
    val sourcePath = args("SOURCE_PATH")             // Example: "s3://136731789529-fis-raw-data/transaction/*/raw/parquet/*"
    val targetPath = args("TARGET_PATH")             // Example: "s3://your-output-bucket/processed/target_data/"
    val batchId = args("BATCH_ID")                   // Example: "2023-10-23_13-03-31-472"
    val outputBucket = args("OUTPUT_BUCKET")         // Example: "s3://your-output-bucket/processed/"

    // Load transaction data from S3 using the source path
    val transactionDateSource = spark.read.parquet(sourcePath)
    val transactionCount = transactionDateSource.count()  // Count total rows in the transaction dataset

    // Select specific columns from the loaded data
    val source = transactionDateSource.select(
        $"customer_key",
        $"account_key",
        $"execution_date",
        $"transaction_date",
        $"original_amount",
        $"original_currency_code",
        $"transaction_location",
        $"transaction_country",
        $"transaction_country_iso_code",
        $"narrative",
        $"shortnarrative",
        $"transaction_type",
        $"txn_direction",
        $"txn_direction_desc",
        $"transaction_type_desc",
        $"amount",
        $"transaction_id",
        $"transaction_code",
        $"transaction_code_desc",
        $"sterling_equivalent",
        $"int_dollar_equivalent",
        $"transaction_routing_number",
        $"external_account_number",
        $"counterparty_customer_key",
        $"counterparty_product_key",
        $"counterparty_id",
        $"source_transaction_id",
        $"branch_key",
        $"account_bucket",
        $"jurisdiction",
        $"screening_system",
        $"source_system",
        $"EFF_START_DATE"
    )

    // Read another transaction source data
    val transactionSource = spark.read.parquet(sourcePath)
    val transactionSourceCount = transactionSource.count()  // Count total rows in the new transaction dataset

    // Filter audit summary data using the batch ID
    val auditSummary = spark.read.parquet(s"${outputBucket}auditSummary/quantexaAuditSummary.parquet/*")
      .filter($"batchId" isin (batchId))
      .filter(!($"processingStage" isin ("InputRawToPartition", "S3RawLayer")))

    // Reading and processing target source
    val target = transactionDateSource.select(
        $"customer_key",
        $"account_key",
        $"account_id",
        $"account_sk",
        $"execution_date",
        $"transaction_date",
        $"original_amount",
        $"original_currency_code",
        $"transaction_location",
        $"transaction_country_iso_code",
        $"narrative",
        $"shortnarrative",
        $"transaction_type",
        $"txn_direction",
        $"txn_direction_desc",
        $"transaction_type_desc",
        $"amount",
        $"transaction_id",
        $"transaction_code",
        $"transaction_code_desc",
        $"sterling_equivalent",
        $"int_dollar_equivalent",
        $"transaction_routing_number",
        $"external_account_number",
        $"counterparty_customer_key",
        $"counterparty_product_key",
        $"counterparty_id",
        $"source_transaction_id",
        $"branch_key",
        $"account_bucket",
        $"jurisdiction",
        $"screening_system",
        $"source_system",
        $"EFF_START_DATE"
    )

    // Save the target DataFrame to the specified S3 bucket as a Parquet file
    target.write.mode("overwrite").parquet(targetPath)

    // Log the path where the data is saved
    println(s"Processed data has been saved to: $targetPath")
  }
}