import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, regexp_replace, when, explode

# Initialize Glue job and Spark context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CUSTOMER_PATH', 'PRODUCT_FORTENT_PATH', 'PRODUCT_CORE_PATH', 'DOCUMENT_PATH', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read customer data
def read_customer_data():
    a_cus = spark.read.parquet(args['CUSTOMER_PATH'])\
        .filter("NUM_OF_ACCOUNT >= 0 and (SOURCE_COUNTRY <> 'ZA' Or SOURCE_COUNTRY IS NULL) and customer_key is not null and customer_key not in('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')")\
        .withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    a_cus.createOrReplaceTempView('a_cus')
    a_cus.cache()
    return a_cus

# Read product data
def read_product_data():
    Prod_fortent = spark.read.parquet(args['PRODUCT_FORTENT_PATH'])
    Prod_fortent.createOrReplaceTempView('Prod_fortent')
    Prod_fortent.cache()

    Prod_core = spark.read.parquet(args['PRODUCT_CORE_PATH'])
    Prod_core.createOrReplaceTempView('Prod_core')
    Prod_core.cache()

    # Union operation
    prodJoin = Prod_core.unionByName(Prod_fortent)
    prodJoin.createOrReplaceTempView('prodJoin')
    return prodJoin

# Scenario 1: Source to Target Comparison (already implemented)
# Scenario 2: Source to Target Comparison (already implemented)
# Scenario 3: Source to Target Comparison (already implemented)
# Scenario 4: Source to Target Comparison (already implemented)
# Scenario 5: Source to Target Comparison (already implemented)

# Scenario 6: Comparison
def scenario_6():
    # Step 1: Create the source fields
    src_fields6 = spark.sql("""
        SELECT customerUniqueId, 
               CAST(maturityDate AS DATE) as maturityDate, 
               CASE WHEN nonOperatingEntity IN ('T', 'true', 'TRUE') THEN 'true' ELSE 'false' END as nonOperatingEntity 
        FROM src_fields
    """).dropDuplicates()
    src_fields6.createOrReplaceTempView('src_fields6')
    src_fields6.cache()

    # Step 2: Create the target fields
    targ_fields6 = spark.sql("""
        SELECT customerUniqueId, 
               maturityDate, 
               CAST(nonOperatingEntity AS STRING) as nonOperatingEntity 
        FROM Qxta_targt
    """).dropDuplicates()
    targ_fields6.createOrReplaceTempView('targ_fields6')
    targ_fields6.cache()

    # Compare source to target
    src_to_tgt_diff = src_fields6.exceptAll(targ_fields6)
    
    # Compare target to source
    tgt_to_src_diff = targ_fields6.exceptAll(src_fields6)

    # Display differences for review
    src_to_tgt_diff.show(10, False)
    tgt_to_src_diff.show(10, False)

    # Return the combined result for final output
    combined_comparison_6 = src_to_tgt_diff.unionByName(tgt_to_src_diff)
    return combined_comparison_6

# Combine all scenarios
def combine_scenarios():
    # Get data sources
    a_cus = read_customer_data()
    prod_qtx = read_product_data()

    # Run each scenario
    comparison_1 = scenario_1(a_cus, prod_qtx)  # Assuming scenario_1 is already defined
    comparison_2 = scenario_2()  # Assuming scenario_2 is already defined
    comparison_3 = scenario_3()  # Assuming scenario_3 is already defined
    comparison_4 = scenario_4()  # Assuming scenario_4 is already defined
    comparison_5 = scenario_5()  # Assuming scenario_5 is already defined
    comparison_6 = scenario_6()

    # Combine all scenario results side by side
    combined_comparisons = comparison_1.unionByName(comparison_2)\
        .unionByName(comparison_3)\
        .unionByName(comparison_4)\
        .unionByName(comparison_5)\
        .unionByName(comparison_6)

    # Write the final combined result to Parquet
    combined_comparisons.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# Run the combination of scenarios
combine_scenarios()

# Commit the Glue job
job.commit()



import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, regexp_replace, when, explode

# Initialize Glue job and Spark context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CUSTOMER_PATH', 'PRODUCT_FORTENT_PATH', 'PRODUCT_CORE_PATH', 'DOCUMENT_PATH', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read customer data
def read_customer_data():
    a_cus = spark.read.parquet(args['CUSTOMER_PATH'])\
        .filter("NUM_OF_ACCOUNT >= 0 and (SOURCE_COUNTRY <> 'ZA' Or SOURCE_COUNTRY IS NULL) and customer_key is not null and customer_key not in('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')")\
        .withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    a_cus.createOrReplaceTempView('a_cus')
    a_cus.cache()
    return a_cus

# Read product data
def read_product_data():
    Prod_fortent = spark.read.parquet(args['PRODUCT_FORTENT_PATH'])
    Prod_fortent.createOrReplaceTempView('Prod_fortent')
    Prod_fortent.cache()

    Prod_core = spark.read.parquet(args['PRODUCT_CORE_PATH'])
    Prod_core.createOrReplaceTempView('Prod_core')
    Prod_core.cache()

    # Union operation
    prodJoin = Prod_core.unionByName(Prod_fortent)
    prodJoin.createOrReplaceTempView('prodJoin')
    return prodJoin

# Scenario 1: Source to Target Comparison (already implemented)
# Scenario 2: Source to Target Comparison (already implemented)
# Scenario 3: Source to Target Comparison (already implemented)
# Scenario 4: Source to Target Comparison (already implemented)

# Scenario 5: Comparison
def scenario_5():
    # Step 1: Create the source fields
    src_fields5 = spark.sql("""
        SELECT customerUniqueId, CAST(update_tms AS DATE) as lastUpdatedDate 
        FROM prod_qtx
    """).dropDuplicates()
    src_fields5.createOrReplaceTempView('src_fields5')
    src_fields5.cache()

    # Step 2: Create the target fields
    targ_fields5 = spark.sql("""
        SELECT customerUniqueId, lastUpdatedDate 
        FROM Qxta_targt
    """).dropDuplicates()
    targ_fields5.createOrReplaceTempView('targ_fields5')
    targ_fields5.cache()

    # Compare source to target
    src_to_tgt_diff = src_fields5.exceptAll(targ_fields5)
    
    # Compare target to source
    tgt_to_src_diff = targ_fields5.exceptAll(src_fields5)

    # Display differences for review
    src_to_tgt_diff.show(10, False)
    tgt_to_src_diff.show(10, False)

    # Return the combined result for final output
    combined_comparison_5 = src_to_tgt_diff.unionByName(tgt_to_src_diff)
    return combined_comparison_5

# Combine all scenarios
def combine_scenarios():
    # Get data sources
    a_cus = read_customer_data()
    prod_qtx = read_product_data()

    # Run each scenario
    comparison_1 = scenario_1(a_cus, prod_qtx)  # Assuming scenario_1 is already defined
    comparison_2 = scenario_2()  # Assuming scenario_2 is already defined
    comparison_3 = scenario_3()  # Assuming scenario_3 is already defined
    comparison_4 = scenario_4()  # Assuming scenario_4 is already defined
    comparison_5 = scenario_5()

    # Combine all scenario results side by side
    combined_comparisons = comparison_1.unionByName(comparison_2).unionByName(comparison_3).unionByName(comparison_4).unionByName(comparison_5)
    
    # You can continue adding more scenario combinations here
    # combined_comparisons = combined_comparisons.unionByName(scenario_6())

    # Write the final combined result to Parquet
    combined_comparisons.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# Run the combination of scenarios
combine_scenarios()

# Commit the Glue job
job.commit()



import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, regexp_replace, when, explode

# Initialize Glue job and Spark context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CUSTOMER_PATH', 'PRODUCT_FORTENT_PATH', 'PRODUCT_CORE_PATH', 'DOCUMENT_PATH', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read customer data
def read_customer_data():
    a_cus = spark.read.parquet(args['CUSTOMER_PATH'])\
        .filter("NUM_OF_ACCOUNT >= 0 and (SOURCE_COUNTRY <> 'ZA' Or SOURCE_COUNTRY IS NULL) and customer_key is not null and customer_key not in('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')")\
        .withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    a_cus.createOrReplaceTempView('a_cus')
    a_cus.cache()
    return a_cus

# Read product data
def read_product_data():
    Prod_fortent = spark.read.parquet(args['PRODUCT_FORTENT_PATH'])
    Prod_fortent.createOrReplaceTempView('Prod_fortent')
    Prod_fortent.cache()

    Prod_core = spark.read.parquet(args['PRODUCT_CORE_PATH'])
    Prod_core.createOrReplaceTempView('Prod_core')
    Prod_core.cache()

    # Union operation
    prodJoin = Prod_core.unionByName(Prod_fortent)
    prodJoin.createOrReplaceTempView('prodJoin')
    return prodJoin

# Scenario 1: Source to Target Comparison (already implemented)
# Scenario 2: Source to Target Comparison (already implemented)
# Scenario 3: Source to Target Comparison (already implemented)

# Scenario 4: Comparison
def scenario_4():
    # Step 1: Create the source fields
    src_fields4 = spark.sql("""
        SELECT customerUniqueId, lineOfBusiness, null as lineOfBusinessDescription 
        FROM src_fields
    """).dropDuplicates()
    src_fields4.createOrReplaceTempView('src_fields4')
    src_fields4.cache()

    # Step 2: Create the target fields
    targ_fields4 = spark.sql("""
        SELECT customerUniqueId, lineOfBusiness, lineOfBusinessDescription 
        FROM selected_Qxta_targt
    """).dropDuplicates()
    targ_fields4.createOrReplaceTempView('targ_fields4')
    targ_fields4.cache()

    # Compare source to target
    src_to_tgt_diff = src_fields4.exceptAll(targ_fields4)
    
    # Compare target to source
    tgt_to_src_diff = targ_fields4.exceptAll(src_fields4)

    # Display differences for review
    src_to_tgt_diff.show(10, False)
    tgt_to_src_diff.show(10, False)

    # Return the combined result for final output
    combined_comparison_4 = src_to_tgt_diff.unionByName(tgt_to_src_diff)
    return combined_comparison_4

# Combine all scenarios
def combine_scenarios():
    # Get data sources
    a_cus = read_customer_data()
    prod_qtx = read_product_data()

    # Run each scenario
    comparison_1 = scenario_1(a_cus, prod_qtx)  # Assuming scenario_1 is already defined
    comparison_2 = scenario_2()  # Assuming scenario_2 is already defined
    comparison_3 = scenario_3()  # Assuming scenario_3 is already defined
    comparison_4 = scenario_4()

    # Combine all scenario results side by side
    combined_comparisons = comparison_1.unionByName(comparison_2).unionByName(comparison_3).unionByName(comparison_4)
    
    # You can continue adding more scenario combinations here
    # combined_comparisons = combined_comparisons.unionByName(scenario_5())

    # Write the final combined result to Parquet
    combined_comparisons.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# Run the combination of scenarios
combine_scenarios()

# Commit the Glue job
job.commit()


import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, regexp_replace, when, explode

# Initialize Glue job and Spark context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CUSTOMER_PATH', 'PRODUCT_FORTENT_PATH', 'PRODUCT_CORE_PATH', 'DOCUMENT_PATH', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read customer data
def read_customer_data():
    a_cus = spark.read.parquet(args['CUSTOMER_PATH'])\
        .filter("NUM_OF_ACCOUNT >= 0 and (SOURCE_COUNTRY <> 'ZA' Or SOURCE_COUNTRY IS NULL) and customer_key is not null and customer_key not in('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')")\
        .withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    a_cus.createOrReplaceTempView('a_cus')
    a_cus.cache()
    return a_cus

# Read product data
def read_product_data():
    Prod_fortent = spark.read.parquet(args['PRODUCT_FORTENT_PATH'])
    Prod_fortent.createOrReplaceTempView('Prod_fortent')
    Prod_fortent.cache()

    Prod_core = spark.read.parquet(args['PRODUCT_CORE_PATH'])
    Prod_core.createOrReplaceTempView('Prod_core')
    Prod_core.cache()

    # Union operation
    prodJoin = Prod_core.unionByName(Prod_fortent)
    prodJoin.createOrReplaceTempView('prodJoin')
    return prodJoin

# Scenario 1: Source to Target Comparison (already implemented)
# Scenario 2: Source to Target Comparison (already implemented)

# Scenario 3: Comparison
def scenario_3():
    # Step 1: Create the source fields for country comparison
    src_fields3a = spark.sql("""
        SELECT customerUniqueId, 
               CASE WHEN country = '*' THEN null ELSE country END as country 
        FROM src_fields
    """).dropDuplicates()
    src_fields3a.createOrReplaceTempView('src_fields3a')
    src_fields3a.cache()

    # Step 2: Create the target fields for country comparison
    targ_fields3b = spark.sql("""
        SELECT customerUniqueId, country 
        FROM selected_Qxta_targt
    """).dropDuplicates()
    targ_fields3b.createOrReplaceTempView('targ_fields3b')
    targ_fields3b.cache()

    # Compare source to target for country
    src_to_tgt_country_diff = src_fields3a.exceptAll(targ_fields3b)
    
    # Compare target to source for country
    tgt_to_src_country_diff = targ_fields3b.exceptAll(src_fields3a)

    # Step 3: Create the source fields for currencyCode comparison
    src_fields3b = spark.sql("""
        SELECT customerUniqueId, currencyCode 
        FROM src_fields
    """).dropDuplicates()
    src_fields3b.createOrReplaceTempView('src_fields3b')
    src_fields3b.cache()

    # Step 4: Create the target fields for currencyCode comparison
    targ_fields3c = spark.sql("""
        SELECT customerUniqueId, currencyCode 
        FROM selected_Qxta_targt
    """).dropDuplicates()
    targ_fields3c.createOrReplaceTempView('targ_fields3c')
    targ_fields3c.cache()

    # Compare source to target for currencyCode
    src_to_tgt_currency_diff = src_fields3b.exceptAll(targ_fields3c)

    # Compare target to source for currencyCode
    tgt_to_src_currency_diff = targ_fields3c.exceptAll(src_fields3b)

    # Step 5: Create the source fields for financialInstitution comparison
    src_fields3 = spark.sql("""
        SELECT customerUniqueId, 
               CASE WHEN financial_institution IN ('N', 'D') THEN 'false' 
                    WHEN financial_institution = 'Y' THEN 'true' 
                    ELSE 'null' 
               END as financial_institution 
        FROM src_fields
    """).dropDuplicates()
    src_fields3.createOrReplaceTempView('src_fields3')
    src_fields3.cache()

    # Step 6: Create the target fields for financialInstitution comparison
    targ_fields3 = spark.sql("""
        SELECT customerUniqueId, 
               CAST(financial_institution AS STRING) as financial_institution 
        FROM selected_Qxta_targt
    """).dropDuplicates()
    targ_fields3.createOrReplaceTempView('targ_fields3')
    targ_fields3.cache()

    # Compare source to target for financialInstitution
    src_to_tgt_financial_diff = src_fields3.exceptAll(targ_fields3)

    # Compare target to source for financialInstitution
    tgt_to_src_financial_diff = targ_fields3.exceptAll(src_fields3)

    # Combine the results for all comparisons in Scenario 3
    combined_scenario_3 = src_to_tgt_country_diff \
        .unionByName(tgt_to_src_country_diff) \
        .unionByName(src_to_tgt_currency_diff) \
        .unionByName(tgt_to_src_currency_diff) \
        .unionByName(src_to_tgt_financial_diff) \
        .unionByName(tgt_to_src_financial_diff)
    
    return combined_scenario_3

# Combine all scenarios
def combine_scenarios():
    # Get data sources
    a_cus = read_customer_data()
    prod_qtx = read_product_data()

    # Run each scenario
    comparison_1 = scenario_1(a_cus, prod_qtx)  # Assuming scenario_1 is already defined
    comparison_2 = scenario_2()  # Assuming scenario_2 is already defined
    comparison_3 = scenario_3()

    # Combine all scenario results side by side
    combined_comparisons = comparison_1.unionByName(comparison_2).unionByName(comparison_3)
    
    # You can continue adding more scenario combinations here
    # combined_comparisons = combined_comparisons.unionByName(scenario_4())

    # Write the final combined result to Parquet
    combined_comparisons.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# Run the combination of scenarios
combine_scenarios()

# Commit the Glue job
job.commit()





import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, regexp_replace, when, explode

# Initialize Glue job and Spark context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CUSTOMER_PATH', 'PRODUCT_FORTENT_PATH', 'PRODUCT_CORE_PATH', 'DOCUMENT_PATH', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read customer data
def read_customer_data():
    a_cus = spark.read.parquet(args['CUSTOMER_PATH'])\
        .filter("NUM_OF_ACCOUNT >= 0 and (SOURCE_COUNTRY <> 'ZA' Or SOURCE_COUNTRY IS NULL) and customer_key is not null and customer_key not in('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')")\
        .withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    a_cus.createOrReplaceTempView('a_cus')
    a_cus.cache()
    return a_cus

# Read product data
def read_product_data():
    Prod_fortent = spark.read.parquet(args['PRODUCT_FORTENT_PATH'])
    Prod_fortent.createOrReplaceTempView('Prod_fortent')
    Prod_fortent.cache()

    Prod_core = spark.read.parquet(args['PRODUCT_CORE_PATH'])
    Prod_core.createOrReplaceTempView('Prod_core')
    Prod_core.cache()

    # Union operation
    prodJoin = Prod_core.unionByName(Prod_fortent)
    prodJoin.createOrReplaceTempView('prodJoin')
    return prodJoin

# Scenario 1: Source to Target Comparison (already implemented)

# Scenario 2: Comparison
def scenario_2():
    # Create the source fields
    src_fields2 = spark.sql("""
        SELECT customerUniqueId, null as cleansedAccountNumber, accountNumber, accountKey, 
               CASE WHEN companyId = '*' THEN null ELSE companyId END as companyId 
        FROM src_fields
    """).dropDuplicates()
    src_fields2.createOrReplaceTempView('src_fields2')
    src_fields2.cache()

    # Create the target fields
    targ_fields2 = spark.sql("""
        SELECT customerUniqueId, cleansedAccountNumber, accountNumber, accountKey, companyId 
        FROM selected_Qxta_targt
    """).dropDuplicates()
    targ_fields2.createOrReplaceTempView('targ_fields2')
    targ_fields2.cache()

    # Compare source to target
    src_to_tgt_diff = src_fields2.exceptAll(targ_fields2)
    
    # Compare target to source
    tgt_to_src_diff = targ_fields2.exceptAll(src_fields2)

    # Display the differences for review (can be adjusted for logging in CloudWatch if needed)
    src_to_tgt_diff.show(10, False)
    tgt_to_src_diff.show(10, False)

    # Return the combined result for the final output
    combined_comparison_2 = src_to_tgt_diff.unionByName(tgt_to_src_diff)
    return combined_comparison_2

# Combine all scenarios
def combine_scenarios():
    # Get data sources
    a_cus = read_customer_data()
    prod_qtx = read_product_data()

    # Run each scenario
    comparison_1 = scenario_1(a_cus, prod_qtx)  # Assuming scenario_1 is already defined
    comparison_2 = scenario_2()

    # Combine all scenario results side by side
    combined_comparisons = comparison_1.unionByName(comparison_2)
    
    # You can continue adding more scenario combinations here
    # combined_comparisons = combined_comparisons.unionByName(scenario_3())

    # Write the final combined result to Parquet
    combined_comparisons.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# Run the combination of scenarios
combine_scenarios()

# Commit the Glue job
job.commit()




import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, regexp_replace, when, explode

# Initialize Glue job and Spark context
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CUSTOMER_PATH', 'PRODUCT_FORTENT_PATH', 'PRODUCT_CORE_PATH', 'DOCUMENT_PATH', 'OUTPUT_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read customer data
def read_customer_data():
    a_cus = spark.read.parquet(args['CUSTOMER_PATH'])\
        .filter("NUM_OF_ACCOUNT >= 0 and (SOURCE_COUNTRY <> 'ZA' Or SOURCE_COUNTRY IS NULL) and customer_key is not null and customer_key not in('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')")\
        .withColumn('customerUniqueId', regexp_replace(col('customer_key'), '^-', ''))
    a_cus.createOrReplaceTempView('a_cus')
    a_cus.cache()
    return a_cus

# Read product data
def read_product_data():
    Prod_fortent = spark.read.parquet(args['PRODUCT_FORTENT_PATH'])
    Prod_fortent.createOrReplaceTempView('Prod_fortent')
    Prod_fortent.cache()

    Prod_core = spark.read.parquet(args['PRODUCT_CORE_PATH'])
    Prod_core.createOrReplaceTempView('Prod_core')
    Prod_core.cache()

    # Union operation
    prodJoin = Prod_core.unionByName(Prod_fortent)
    prodJoin.createOrReplaceTempView('prodJoin')
    return prodJoin

# Scenario 1: Source to Target Comparison
def scenario_1(src_fields1, targ_fields1):
    # Compare from source to target
    src_to_tgt_diff = src_fields1.exceptAll(targ_fields1)
    src_to_tgt_diff = src_to_tgt_diff.withColumnRenamed("field", "source_to_target_diff")
    
    # Compare from target to source
    tgt_to_src_diff = targ_fields1.exceptAll(src_fields1)
    tgt_to_src_diff = tgt_to_src_diff.withColumnRenamed("field", "target_to_source_diff")
    
    # Combine results side by side
    comparison_1 = src_to_tgt_diff.unionByName(tgt_to_src_diff)
    return comparison_1

# Scenario 2: (Another example scenario)
def scenario_2(prod_qtx):
    # Example comparison on account numbers
    comparison_2 = prod_qtx.select("customerUniqueId", "accountNumber").distinct()
    return comparison_2

# Additional scenario functions can be defined here...
# def scenario_3(): ...

# Combine all scenarios
def combine_scenarios():
    # Get data sources
    a_cus = read_customer_data()
    prod_qtx = read_product_data()

    # Run each scenario
    comparison_1 = scenario_1(a_cus, prod_qtx)  # You can pass relevant dataframes to each function
    comparison_2 = scenario_2(prod_qtx)

    # Combine all scenario results side by side
    combined_comparisons = comparison_1.unionByName(comparison_2)
    
    # You can continue adding more scenario combinations here
    # combined_comparisons = combined_comparisons.unionByName(scenario_3())

    # Write the final combined result to Parquet
    combined_comparisons.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# Run the combination of scenarios
combine_scenarios()

# Commit the Glue job
job.commit()











