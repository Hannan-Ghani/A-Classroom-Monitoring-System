To meet your requirements, the Glue Job can be modified to write the output as a .txt file that includes the number of differences and the customerUniqueId values where differences are found. Here’s the updated Glue Job script:

Updated Glue Job Script

import sys
import boto3
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_replace, lit, when, explode
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from io import StringIO

# Parse arguments passed to the Glue Job
args = getResolvedOptions(sys.argv, ["JOB_NAME", "config_file", "columns_file", "output_file"])
config_file = args["config_file"]  # Path to the main config file in S3
columns_file = args["columns_file"]  # Path to the columns config file in S3
output_file = args["output_file"]  # Path for the differences summary file in S3

# Initialize Glue and Spark contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = glueContext.create_job(args["JOB_NAME"])

# Helper function to read a file from S3
def read_s3_file(file_path):
    s3 = boto3.client("s3")
    bucket, key = file_path.replace("s3://", "").split("/", 1)
    return s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")

# Helper function to write a string to S3 as a .txt file
def write_s3_file(content, file_path):
    s3 = boto3.client("s3")
    bucket, key = file_path.replace("s3://", "").split("/", 1)
    s3.put_object(Body=content, Bucket=bucket, Key=key)

# Read and parse the main config file
config_data = read_s3_file(config_file)
config = {line.split("=")[0].strip(): line.split("=")[1].strip() for line in config_data.split("\n") if "=" in line}

# Read and parse the columns config file
columns_data = read_s3_file(columns_file)
columns_config = {line.split("=")[0].strip(): line.split("=")[1].strip() for line in columns_data.split("\n") if "=" in line}

# Extract file paths from the main config
customer_path = config["customer_path"]
product_path = config["product_path"]
target_path = config["target_path"]
output_path = config["output_path"]

# Extract column configurations
columns_src = columns_config["columns_src"].split(",")  # Columns for src_fields
columns_trgt = columns_config["columns_trgt"].split(",")  # Columns for targ_fields

# Read customer data
a_cus = (
    spark.read.parquet(customer_path)
    .filter(
        "NUM_OF_ACCOUNT >= 0 AND "
        "(SOURCE_COUNTRY <> 'ZA' OR SOURCE_COUNTRY IS NULL) AND "
        "customer_key IS NOT NULL AND "
        "customer_key NOT IN ('', '***', 'TTMAMC-**', 'TMEMA-**', 'TMUK1-**', 'MUK2-**', 'Not Available')"
    )
    .withColumn(
        "customerUniqueId", 
        regexp_replace(col("customer_key"), "^.*-", "")  # Clean up 'customer_key'
    )
)

# Read product data
a_prod = (
    spark.read.parquet(product_path)
    .filter(
        (col("account_key").isNotNull() | col("customer_key").isNotNull()) &
        ~(
            (col("screening_system") == "YMUK2") &
            (
                (~col("customer_role").isin("CML - COMP PARENT", "PRIMARY")) |
                col("low_account_type").isin(
                    "UKCMLPHVI", "UKCMLPV100", "UKCMLPV5K", "UKCMLPV500", 
                    "UKCMLPV10K", "UKCMLPV1K", "UKCMLPVUNK", "UKCMLPBHI", 
                    "UKCMLPB100", "UKCMLPB5K", "UKCMLPB500", "UKCMLPB1K", "UKCMLPBUNK"
                )
            )
        )
    )
    .withColumn(
        "customerUniqueId", 
        regexp_replace(col("customer_key"), "^.*-", "")  # Clean up 'customer_key'
    )
)

# Join customer and product data
prod_qtx = (
    a_cus.join(a_prod, a_cus.customerUniqueId == a_prod.customerUniqueId, how="inner")
    .select(a_prod["*"])
)

# Read and process the target data
Qxta_targt = (
    spark.read.parquet(target_path)
    .select(
        "customerUniqueId", 
        explode(col("account")).alias("account")  # Explode the 'account' array
    )
    .select(
        "customerUniqueId", 
        "account.*"  # Expand all fields from the exploded 'account' struct
    )
)

# Select required fields for src_fields and targ_fields dynamically
src_fields = prod_qtx.select(*[col(field) for field in columns_src])
src_fields.createOrReplaceTempView("src_fields")
src_fields.cache()

selected_Qxta_targt = Qxta_targt.select(*[col(field) for field in columns_trgt])
selected_Qxta_targt.createOrReplaceTempView("selected_Qxta_targt")
selected_Qxta_targt.cache()

# Compare src_fields1 and targ_fields1
src_fields1 = src_fields.dropDuplicates()
targ_fields1 = selected_Qxta_targt.dropDuplicates()

# Find differences
src_diff = src_fields1.exceptAll(targ_fields1)
targ_diff = targ_fields1.exceptAll(src_fields1)

# Extract customerUniqueIds with differences
src_diff_ids = src_diff.select("customerUniqueId").rdd.flatMap(lambda x: x).collect()
targ_diff_ids = targ_diff.select("customerUniqueId").rdd.flatMap(lambda x: x).collect()

# Prepare summary content
summary_content = []

if src_diff_ids:
    summary_content.append(f"Differences Scenario 1: Differences found {len(src_diff_ids)}")
    summary_content.extend(src_diff_ids)

if targ_diff_ids:
    summary_content.append(f"Differences Scenario 1 (Target): Differences found {len(targ_diff_ids)}")
    summary_content.extend(targ_diff_ids)

# Write summary to S3
write_s3_file("\n".join(summary_content), output_file)

# Commit Glue job
job.commit()

How It Works
	1.	Write Differences to Text File:
	•	The differences are written as a .txt file to the specified S3 path using the write_s3_file() function.
	2.	Content Structure:
	•	For each scenario, the summary starts with:

Differences Scenario 1: Differences found 2


	•	Followed by the customerUniqueId values where differences are found.

	3.	Dynamic Content:
	•	The Glue Job dynamically creates and writes the content for each scenario based on the differences identified.
	4.	Arguments:
	•	--output_file: Path to the .txt file in S3 where the differences summary will be written.

Example Differences Summary (output_file)

Content of the generated .txt file in S3:

Differences Scenario 1: Differences found 2
CUS12345
CUS67890
Differences Scenario 1 (Target): Differences found 1
CUS54321

Example Parameters for Glue Job

--JOB_NAME MyGlueJob
--config_file s3://your-config-bucket/config.txt
--columns_file s3://your-config-bucket/columns_config.txt
--output_file s3://your-output-bucket/differences_summary.txt

Next Steps
	1.	Upload Config Files:
	•	Ensure config.txt and columns_config.txt are in S3.
	2.	Run the Glue Job:
	•	Execute the Glue Job with the necessary parameters.
	3.	Validate the Output:
	•	Check the .txt file in the specified output_file location to verify the summary of differences.