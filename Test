import com.amazonaws.services.glue.GlueContext
import com.amazonaws.services.glue.util.GlueArgParser
import com.amazonaws.services.glue.util.Job
import org.apache.spark.SparkContext
import org.apache.spark.sql.functions._
import org.apache.spark.sql.{SaveMode, SparkSession}

object GlueApp {
  def main(sysArgs: Array[String]): Unit = {
    // Initialize GlueContext and SparkSession
    val sc: SparkContext = new SparkContext()
    val glueContext: GlueContext = new GlueContext(sc)
    val spark: SparkSession = glueContext.getSparkSession

    // Parse arguments
    val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("sourcePath", "targetPath", "outputPath", "configPath", "JOB_NAME").toArray)
    Job.init(args("JOB_NAME"), glueContext, args.asJava)

    // Paths
    val sourcePath = args("sourcePath")
    val targetPath = args("targetPath")
    val outputPath = args("outputPath")
    val configPath = args("configPath")

    // Read source, target, and config data
    val transactionSource = spark.read.parquet(s"$sourcePath")
    val target = spark.read.parquet(s"$targetPath")
    val configDF = spark.read.json(s"$configPath")

    // Convert config JSON to a map
    val configData = configDF.collect().map(row => {
      val rules = row.getAs[Seq[org.apache.spark.sql.Row]]("rules").map(rule => {
        Map(
          "source_column" -> rule.getAs[String]("source_column"),
          "target_column" -> rule.getAs[String]("target_column"),
          "transformation" -> rule.getAs[String]("transformation"),
          "type" -> rule.getAs[String]("type")
        )
      })
      Map("rules" -> rules)
    }).headOption.getOrElse(throw new Exception("Configuration data is missing or invalid."))

    // Apply transformations based on config data
    var transformedSourceDF = transactionSource

    for (rule <- configData("rules").asInstanceOf[List[Map[String, Any]]]) {
      val sourceCol = rule("source_column").toString
      val targetCol = rule("target_column").toString

      rule("transformation").toString match {
        case "copy" =>
          transformedSourceDF = transformedSourceDF.withColumn(targetCol, col(sourceCol))

        case "cast" if rule("type") == "double" =>
          transformedSourceDF = transformedSourceDF.withColumn(targetCol, col(sourceCol).cast("double"))

        case "concat" =>
          val additionalCol = rule.get("additional_source_column").map(_.toString).getOrElse("")
          val separator = rule.get("separator").map(_.toString).getOrElse("")
          transformedSourceDF = transformedSourceDF.withColumn(targetCol, concat_ws(separator, col(sourceCol), col(additionalCol)))

        case "conditional" =>
          val conditionField = rule("condition").asInstanceOf[Map[String, String]]("field")
          val conditionValue = rule("condition").asInstanceOf[Map[String, String]]("value")
          val elseValue = rule("condition").asInstanceOf[Map[String, String]]("else_value")
          transformedSourceDF = transformedSourceDF.withColumn(targetCol, when(col(conditionField) === conditionValue, col(sourceCol)).otherwise(col(elseValue)))
      }
    }

    // Select columns to compare
    val sourceColumns = transformedSourceDF.select("transactionId", "narrative").as("source")
    val targetColumns = target.select("transactionId", "narrative").as("target")

    // Compare source and target data
    val dataDifference = sourceColumns.exceptAll(targetColumns).limit(10)

    // Write output to parquet
    dataDifference.write.format("parquet").mode(SaveMode.Overwrite).save(s"$outputPath")

    // Commit the job
    Job.commit()
  }
}